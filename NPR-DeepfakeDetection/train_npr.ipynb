{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:24:11.592021Z",
     "start_time": "2024-05-16T00:24:10.089181Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/bmsn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "from validate import validate\n",
    "from data import create_dataloader\n",
    "from networks.trainer import Trainer\n",
    "from options.train_options import TrainOptions\n",
    "from options.test_options import TestOptions\n",
    "from util import Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c22823b8328957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:24:11.595849Z",
     "start_time": "2024-05-16T00:24:11.593621Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c7e4e7d78a57ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:24:11.601603Z",
     "start_time": "2024-05-16T00:24:11.599543Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_val_opt():\n",
    "    val_opt = TrainOptions().parse(print_options=True)\n",
    "    val_opt.dataroot = '{}/{}/'.format(val_opt.dataroot, val_opt.val_split)\n",
    "    val_opt.isTrain = False\n",
    "    val_opt.no_resize = False\n",
    "    val_opt.no_crop = False\n",
    "    val_opt.serial_batches = True\n",
    "\n",
    "    return val_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f90e7bfbec6ec1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:24:12.973387Z",
     "start_time": "2024-05-16T00:24:11.605993Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/bmsn/lib/python3.10/site-packages/eth_utils/network.py:44: UserWarning: Network 345 with name 'Yooldo Verse Mainnet' does not have a valid ChainId. eth-typing should be updated with the latest networks.\n",
      "  warnings.warn(\n",
      "/home/user/mambaforge/envs/bmsn/lib/python3.10/site-packages/eth_utils/network.py:44: UserWarning: Network 12611 with name 'Astar zkEVM' does not have a valid ChainId. eth-typing should be updated with the latest networks.\n",
      "  warnings.warn(\n",
      "Downloading data: 100%|██████████| 963/963 [00:00<00:00, 203502.36files/s]\n",
      "Generating train split: 963 examples [00:00, 16774.36 examples/s]\n",
      "Downloading data: 100%|██████████| 115/115 [00:00<00:00, 106902.70files/s]\n",
      "Generating train split: 115 examples [00:00, 12124.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from bitmind.real_fake_dataset import RealFakeDataset\n",
    "from bitmind.real_image_dataset import RealImageDataset\n",
    "#from bitmind.random_image_generator import RandomImageGenerator\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "train_real_image_dataset = RealImageDataset(huggingface_dataset_names=['dalle-mini/open-images'], splits=['train'])\n",
    "val_real_image_dataset = RealImageDataset(huggingface_dataset_names=['dalle-mini/open-images'], splits=['validation'])\n",
    "test_real_image_dataset = RealImageDataset(huggingface_dataset_names=['dalle-mini/open-images'], splits=['test'])\n",
    "\n",
    "train_fake_image_dataset = RealImageDataset(huggingface_dataset_names=['imagefolder:../bitmind/data/images/train'])\n",
    "val_fake_image_dataset = RealImageDataset(huggingface_dataset_names=['imagefolder:../bitmind/data/images/val'])\n",
    "test_fake_image_dataset = RealImageDataset(huggingface_dataset_names=['imagefolder:../bitmind/data/images/test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac0159b951d28636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:24:12.976951Z",
     "start_time": "2024-05-16T00:24:12.975856Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MEAN = {\n",
    "    \"imagenet\":[0.485, 0.456, 0.406],\n",
    "    \"clip\":[0.48145466, 0.4578275, 0.40821073]\n",
    "}\n",
    "\n",
    "STD = {\n",
    "    \"imagenet\":[0.229, 0.224, 0.225],\n",
    "    \"clip\":[0.26862954, 0.26130258, 0.27577711]\n",
    "}\n",
    "\n",
    "def CenterCrop():\n",
    "    def fn(img):\n",
    "        m = min(img.size)\n",
    "        return transforms.CenterCrop(m)(img)\n",
    "    return fn\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    CenterCrop(),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: t.expand(3, -1, -1) if t.shape[0] == 1 else t),\n",
    "    #transforms.Lambda(lambda t: t.float() / 255.),\n",
    "\n",
    "    #transforms.Normalize( mean=MEAN['imagenet'], std=STD['imagenet'] ),\n",
    "])\n",
    "\n",
    "train_dataset = RealFakeDataset(real_image_dataset=train_real_image_dataset, fake_image_dataset=train_fake_image_dataset, transforms=transform)\n",
    "val_dataset = RealFakeDataset(real_image_dataset=val_real_image_dataset, fake_image_dataset=val_fake_image_dataset, transforms=transform)\n",
    "test_dataset = RealFakeDataset(real_image_dataset=test_real_image_dataset, fake_image_dataset=test_fake_image_dataset, transforms=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6813261ee333543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:24:13.007372Z",
     "start_time": "2024-05-16T00:24:12.984939Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                     arch: res50                         \n",
      "               batch_size: 64                            \n",
      "                    beta1: 0.9                           \n",
      "                blur_prob: 0                             \n",
      "                 blur_sig: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                class_bal: False                         \n",
      "                  classes:                               \n",
      "           continue_train: False                         \n",
      "                 cropSize: 224                           \n",
      "                 data_aug: False                         \n",
      "                 dataroot: ./dataset/                    \n",
      "                delr_freq: 20                            \n",
      "          earlystop_epoch: 15                            \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                  isTrain: True                          \t[default: None]\n",
      "               jpg_method: cv2                           \n",
      "                 jpg_prob: 0                             \n",
      "                 jpg_qual: 75                            \n",
      "               last_epoch: -1                            \n",
      "                 loadSize: 256                           \n",
      "                loss_freq: 400                           \n",
      "                       lr: 0.0001                        \n",
      "                     mode: binary                        \n",
      "                     name: experiment_name2024_05_18_18_41_05\t[default: experiment_name]\n",
      "                new_optim: False                         \n",
      "                    niter: 1000                          \n",
      "                  no_flip: False                         \n",
      "              num_threads: 8                             \n",
      "                    optim: adam                          \n",
      "           resize_or_crop: scale_and_crop                \n",
      "                rz_interp: bilinear                      \n",
      "          save_epoch_freq: 20                            \n",
      "         save_latest_freq: 2000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "              train_split: train                         \n",
      "                val_split: val                           \n",
      "----------------- End -------------------\n",
      "cwd: /home/user/bitmind-subnet/npr\n",
      "2024_05_18_18_41_24 Train loss: 0.7130370736122131 at step: 1 lr 0.0001\n",
      "2024_05_18_18_41_36 Train loss: 0.65699702501297 at step: 2 lr 0.0001\n",
      "2024_05_18_18_41_41 Train loss: 0.6172909736633301 at step: 3 lr 0.0001\n",
      "2024_05_18_18_41_51 Train loss: 0.5993362069129944 at step: 4 lr 0.0001\n",
      "2024_05_18_18_42_04 Train loss: 0.5934795141220093 at step: 5 lr 0.0001\n",
      "2024_05_18_18_42_14 Train loss: 0.5242716073989868 at step: 6 lr 0.0001\n",
      "2024_05_18_18_42_25 Train loss: 0.5046413540840149 at step: 7 lr 0.0001\n",
      "2024_05_18_18_42_33 Train loss: 0.6135424971580505 at step: 8 lr 0.0001\n",
      "2024_05_18_18_42_41 Train loss: 0.5359221696853638 at step: 9 lr 0.0001\n",
      "2024_05_18_18_42_51 Train loss: 0.4707523286342621 at step: 10 lr 0.0001\n",
      "2024_05_18_18_42_59 Train loss: 0.5385260581970215 at step: 11 lr 0.0001\n",
      "2024_05_18_18_43_11 Train loss: 0.6684766411781311 at step: 12 lr 0.0001\n",
      "2024_05_18_18_43_23 Train loss: 0.7344341278076172 at step: 13 lr 0.0001\n",
      "2024_05_18_18_43_31 Train loss: 0.5866117477416992 at step: 14 lr 0.0001\n",
      "2024_05_18_18_43_40 Train loss: 0.48000335693359375 at step: 15 lr 0.0001\n",
      "2024_05_18_18_43_47 Train loss: 0.6042094230651855 at step: 16 lr 0.0001\n",
      "2024_05_18_18_43_57 Train loss: 0.48510053753852844 at step: 17 lr 0.0001\n",
      "2024_05_18_18_44_07 Train loss: 0.5646635890007019 at step: 18 lr 0.0001\n",
      "2024_05_18_18_44_19 Train loss: 0.504070520401001 at step: 19 lr 0.0001\n",
      "2024_05_18_18_44_27 Train loss: 0.4692882001399994 at step: 20 lr 0.0001\n",
      "2024_05_18_18_44_36 Train loss: 0.45395445823669434 at step: 21 lr 0.0001\n",
      "2024_05_18_18_44_45 Train loss: 0.45037105679512024 at step: 22 lr 0.0001\n",
      "2024_05_18_18_44_51 Train loss: 0.5438559055328369 at step: 23 lr 0.0001\n",
      "2024_05_18_18_44_57 Train loss: 0.5507157444953918 at step: 24 lr 0.0001\n",
      "2024_05_18_18_45_04 Train loss: 0.6137263774871826 at step: 25 lr 0.0001\n",
      "2024_05_18_18_45_15 Train loss: 0.4981078803539276 at step: 26 lr 0.0001\n",
      "2024_05_18_18_45_24 Train loss: 0.48250842094421387 at step: 27 lr 0.0001\n",
      "2024_05_18_18_45_38 Train loss: 0.5616004467010498 at step: 28 lr 0.0001\n",
      "2024_05_18_18_45_48 Train loss: 0.38344722986221313 at step: 29 lr 0.0001\n",
      "2024_05_18_18_45_59 Train loss: 0.42487722635269165 at step: 30 lr 0.0001\n",
      "2024_05_18_18_45_59 Train loss: 0.8249089121818542 at step: 31 lr 0.0001\n",
      "(Val @ epoch 0) acc: 0.4782608695652174; ap: 0.45077641969633747\n",
      "Saving model ./checkpoints/experiment_name2024_05_18_18_41_03/model_epoch_best.pth\n",
      "2024_05_18_18_46_31 Train loss: 0.5076690912246704 at step: 32 lr 0.0001\n",
      "2024_05_18_18_46_38 Train loss: 0.4600604176521301 at step: 33 lr 0.0001\n",
      "2024_05_18_18_46_47 Train loss: 0.5052191615104675 at step: 34 lr 0.0001\n",
      "2024_05_18_18_46_59 Train loss: 0.6545590162277222 at step: 35 lr 0.0001\n",
      "2024_05_18_18_47_08 Train loss: 0.5206996202468872 at step: 36 lr 0.0001\n",
      "2024_05_18_18_47_17 Train loss: 0.3749896287918091 at step: 37 lr 0.0001\n",
      "2024_05_18_18_47_27 Train loss: 0.4625100791454315 at step: 38 lr 0.0001\n",
      "2024_05_18_18_47_34 Train loss: 0.4359598159790039 at step: 39 lr 0.0001\n",
      "2024_05_18_18_47_45 Train loss: 0.49521130323410034 at step: 40 lr 0.0001\n",
      "2024_05_18_18_47_58 Train loss: 0.4136844873428345 at step: 41 lr 0.0001\n",
      "2024_05_18_18_48_09 Train loss: 0.34209662675857544 at step: 42 lr 0.0001\n",
      "2024_05_18_18_48_21 Train loss: 0.5175845623016357 at step: 43 lr 0.0001\n",
      "2024_05_18_18_48_31 Train loss: 0.4397379755973816 at step: 44 lr 0.0001\n",
      "2024_05_18_18_48_40 Train loss: 0.5932402014732361 at step: 45 lr 0.0001\n",
      "2024_05_18_18_48_48 Train loss: 0.32352352142333984 at step: 46 lr 0.0001\n",
      "2024_05_18_18_49_02 Train loss: 0.5826210975646973 at step: 47 lr 0.0001\n",
      "2024_05_18_18_49_12 Train loss: 0.32932066917419434 at step: 48 lr 0.0001\n",
      "2024_05_18_18_49_19 Train loss: 0.36876559257507324 at step: 49 lr 0.0001\n",
      "2024_05_18_18_49_32 Train loss: 0.3649197518825531 at step: 50 lr 0.0001\n",
      "2024_05_18_18_49_39 Train loss: 0.4073578715324402 at step: 51 lr 0.0001\n",
      "2024_05_18_18_49_59 Train loss: 0.23802681267261505 at step: 52 lr 0.0001\n",
      "2024_05_18_18_50_23 Train loss: 0.23973789811134338 at step: 53 lr 0.0001\n",
      "2024_05_18_18_50_41 Train loss: 0.3096373677253723 at step: 54 lr 0.0001\n",
      "2024_05_18_18_50_57 Train loss: 0.41271859407424927 at step: 55 lr 0.0001\n",
      "2024_05_18_18_51_09 Train loss: 0.31688734889030457 at step: 56 lr 0.0001\n",
      "2024_05_18_18_51_17 Train loss: 0.20610469579696655 at step: 57 lr 0.0001\n",
      "2024_05_18_18_51_25 Train loss: 0.35441359877586365 at step: 58 lr 0.0001\n",
      "2024_05_18_18_51_37 Train loss: 0.2776477336883545 at step: 59 lr 0.0001\n",
      "2024_05_18_18_51_48 Train loss: 0.2492992877960205 at step: 60 lr 0.0001\n",
      "2024_05_18_18_51_55 Train loss: 0.24490894377231598 at step: 61 lr 0.0001\n",
      "2024_05_18_18_51_57 Train loss: 0.6240889430046082 at step: 62 lr 0.0001\n",
      "(Val @ epoch 1) acc: 0.5826086956521739; ap: 0.589430640804683\n",
      "Saving model ./checkpoints/experiment_name2024_05_18_18_41_03/model_epoch_best.pth\n",
      "2024_05_18_18_52_24 Train loss: 0.32657676935195923 at step: 63 lr 0.0001\n",
      "2024_05_18_18_52_33 Train loss: 0.3834289610385895 at step: 64 lr 0.0001\n",
      "2024_05_18_18_52_42 Train loss: 0.2739478051662445 at step: 65 lr 0.0001\n",
      "2024_05_18_18_52_52 Train loss: 0.25774580240249634 at step: 66 lr 0.0001\n",
      "2024_05_18_18_53_03 Train loss: 0.5145977735519409 at step: 67 lr 0.0001\n",
      "2024_05_18_18_53_15 Train loss: 0.375454843044281 at step: 68 lr 0.0001\n",
      "2024_05_18_18_53_25 Train loss: 0.4222525656223297 at step: 69 lr 0.0001\n",
      "2024_05_18_18_53_31 Train loss: 0.326166033744812 at step: 70 lr 0.0001\n",
      "2024_05_18_18_53_43 Train loss: 0.2730616331100464 at step: 71 lr 0.0001\n",
      "2024_05_18_18_53_54 Train loss: 0.3631967306137085 at step: 72 lr 0.0001\n",
      "2024_05_18_18_54_04 Train loss: 0.24391669034957886 at step: 73 lr 0.0001\n",
      "2024_05_18_18_54_12 Train loss: 0.20650632679462433 at step: 74 lr 0.0001\n",
      "2024_05_18_18_54_23 Train loss: 0.20632874965667725 at step: 75 lr 0.0001\n",
      "2024_05_18_18_54_33 Train loss: 0.21925419569015503 at step: 76 lr 0.0001\n",
      "2024_05_18_18_54_43 Train loss: 0.31095030903816223 at step: 77 lr 0.0001\n",
      "2024_05_18_18_54_53 Train loss: 0.19847187399864197 at step: 78 lr 0.0001\n",
      "2024_05_18_18_55_02 Train loss: 0.3020590543746948 at step: 79 lr 0.0001\n",
      "2024_05_18_18_55_12 Train loss: 0.35792261362075806 at step: 80 lr 0.0001\n",
      "2024_05_18_18_55_20 Train loss: 0.6126527786254883 at step: 81 lr 0.0001\n",
      "2024_05_18_18_55_30 Train loss: 0.17007890343666077 at step: 82 lr 0.0001\n",
      "2024_05_18_18_55_44 Train loss: 0.533356249332428 at step: 83 lr 0.0001\n",
      "2024_05_18_18_55_53 Train loss: 0.1815829575061798 at step: 84 lr 0.0001\n",
      "2024_05_18_18_56_00 Train loss: 0.22366097569465637 at step: 85 lr 0.0001\n",
      "2024_05_18_18_56_08 Train loss: 0.2536742389202118 at step: 86 lr 0.0001\n",
      "2024_05_18_18_56_16 Train loss: 0.32778024673461914 at step: 87 lr 0.0001\n",
      "2024_05_18_18_56_24 Train loss: 0.37542960047721863 at step: 88 lr 0.0001\n",
      "2024_05_18_18_56_39 Train loss: 0.5473037958145142 at step: 89 lr 0.0001\n",
      "2024_05_18_18_56_50 Train loss: 0.2614709138870239 at step: 90 lr 0.0001\n",
      "2024_05_18_18_56_57 Train loss: 0.19219842553138733 at step: 91 lr 0.0001\n",
      "2024_05_18_18_57_06 Train loss: 0.195451021194458 at step: 92 lr 0.0001\n",
      "2024_05_18_18_57_06 Train loss: 1.1623950004577637 at step: 93 lr 0.0001\n",
      "(Val @ epoch 2) acc: 0.5304347826086957; ap: 0.8207809390367535\n",
      "2024_05_18_18_57_34 Train loss: 0.25160539150238037 at step: 94 lr 0.0001\n",
      "2024_05_18_18_57_51 Train loss: 0.4564170241355896 at step: 95 lr 0.0001\n",
      "2024_05_18_18_58_10 Train loss: 0.5532254576683044 at step: 96 lr 0.0001\n",
      "2024_05_18_18_58_29 Train loss: 0.43305450677871704 at step: 97 lr 0.0001\n",
      "2024_05_18_18_58_39 Train loss: 0.2886248528957367 at step: 98 lr 0.0001\n",
      "2024_05_18_18_58_52 Train loss: 0.2493322640657425 at step: 99 lr 0.0001\n",
      "2024_05_18_18_59_06 Train loss: 0.2453278750181198 at step: 100 lr 0.0001\n",
      "2024_05_18_18_59_17 Train loss: 0.15532803535461426 at step: 101 lr 0.0001\n",
      "2024_05_18_18_59_26 Train loss: 0.1952620893716812 at step: 102 lr 0.0001\n",
      "2024_05_18_18_59_46 Train loss: 0.4030293822288513 at step: 103 lr 0.0001\n",
      "2024_05_18_18_59_58 Train loss: 0.23577828705310822 at step: 104 lr 0.0001\n",
      "2024_05_18_19_00_12 Train loss: 0.3594162166118622 at step: 105 lr 0.0001\n",
      "2024_05_18_19_00_20 Train loss: 0.4242277145385742 at step: 106 lr 0.0001\n",
      "2024_05_18_19_00_29 Train loss: 0.15714994072914124 at step: 107 lr 0.0001\n",
      "2024_05_18_19_00_41 Train loss: 0.20380552113056183 at step: 108 lr 0.0001\n",
      "2024_05_18_19_00_51 Train loss: 0.19663900136947632 at step: 109 lr 0.0001\n",
      "2024_05_18_19_00_57 Train loss: 0.344573974609375 at step: 110 lr 0.0001\n",
      "2024_05_18_19_01_09 Train loss: 0.35221439599990845 at step: 111 lr 0.0001\n",
      "2024_05_18_19_01_19 Train loss: 0.18074852228164673 at step: 112 lr 0.0001\n",
      "2024_05_18_19_01_30 Train loss: 0.3216630816459656 at step: 113 lr 0.0001\n",
      "2024_05_18_19_01_38 Train loss: 0.19320976734161377 at step: 114 lr 0.0001\n",
      "2024_05_18_19_01_47 Train loss: 0.2966541051864624 at step: 115 lr 0.0001\n",
      "2024_05_18_19_01_59 Train loss: 0.22689898312091827 at step: 116 lr 0.0001\n",
      "2024_05_18_19_02_08 Train loss: 0.2922678589820862 at step: 117 lr 0.0001\n",
      "2024_05_18_19_02_23 Train loss: 0.3204902410507202 at step: 118 lr 0.0001\n",
      "2024_05_18_19_02_33 Train loss: 0.17574304342269897 at step: 119 lr 0.0001\n",
      "2024_05_18_19_02_43 Train loss: 0.20718392729759216 at step: 120 lr 0.0001\n",
      "2024_05_18_19_02_52 Train loss: 0.3364804983139038 at step: 121 lr 0.0001\n",
      "2024_05_18_19_03_03 Train loss: 0.14928822219371796 at step: 122 lr 0.0001\n",
      "2024_05_18_19_03_10 Train loss: 0.3248235583305359 at step: 123 lr 0.0001\n",
      "2024_05_18_19_03_11 Train loss: 0.07700248062610626 at step: 124 lr 0.0001\n",
      "(Val @ epoch 3) acc: 0.9304347826086956; ap: 0.9798384676979326\n",
      "Saving model ./checkpoints/experiment_name2024_05_18_18_41_03/model_epoch_best.pth\n",
      "2024_05_18_19_03_43 Train loss: 0.24958203732967377 at step: 125 lr 0.0001\n",
      "2024_05_18_19_03_51 Train loss: 0.2841660678386688 at step: 126 lr 0.0001\n",
      "2024_05_18_19_04_01 Train loss: 0.26277390122413635 at step: 127 lr 0.0001\n",
      "2024_05_18_19_04_16 Train loss: 0.30731114745140076 at step: 128 lr 0.0001\n",
      "2024_05_18_19_04_30 Train loss: 0.23511669039726257 at step: 129 lr 0.0001\n",
      "2024_05_18_19_04_39 Train loss: 0.1604541838169098 at step: 130 lr 0.0001\n",
      "2024_05_18_19_04_48 Train loss: 0.15970377624034882 at step: 131 lr 0.0001\n",
      "2024_05_18_19_04_59 Train loss: 0.21155516803264618 at step: 132 lr 0.0001\n",
      "2024_05_18_19_05_08 Train loss: 0.17263689637184143 at step: 133 lr 0.0001\n",
      "2024_05_18_19_05_17 Train loss: 0.20355162024497986 at step: 134 lr 0.0001\n",
      "2024_05_18_19_05_29 Train loss: 0.3128710389137268 at step: 135 lr 0.0001\n",
      "2024_05_18_19_05_40 Train loss: 0.3086165189743042 at step: 136 lr 0.0001\n",
      "2024_05_18_19_05_51 Train loss: 0.12417278438806534 at step: 137 lr 0.0001\n",
      "2024_05_18_19_06_04 Train loss: 0.13590222597122192 at step: 138 lr 0.0001\n",
      "2024_05_18_19_06_12 Train loss: 0.10238619893789291 at step: 139 lr 0.0001\n",
      "2024_05_18_19_06_28 Train loss: 0.15435443818569183 at step: 140 lr 0.0001\n",
      "2024_05_18_19_06_43 Train loss: 0.3757494390010834 at step: 141 lr 0.0001\n",
      "2024_05_18_19_07_06 Train loss: 0.19901423156261444 at step: 142 lr 0.0001\n",
      "2024_05_18_19_07_26 Train loss: 0.397693395614624 at step: 143 lr 0.0001\n",
      "2024_05_18_19_07_39 Train loss: 0.2241707742214203 at step: 144 lr 0.0001\n",
      "2024_05_18_19_07_50 Train loss: 0.15518100559711456 at step: 145 lr 0.0001\n",
      "2024_05_18_19_08_02 Train loss: 0.17808601260185242 at step: 146 lr 0.0001\n",
      "2024_05_18_19_08_14 Train loss: 0.22381709516048431 at step: 147 lr 0.0001\n",
      "2024_05_18_19_08_26 Train loss: 0.2548551559448242 at step: 148 lr 0.0001\n",
      "2024_05_18_19_08_36 Train loss: 0.12803414463996887 at step: 149 lr 0.0001\n",
      "2024_05_18_19_08_47 Train loss: 0.20205363631248474 at step: 150 lr 0.0001\n",
      "2024_05_18_19_08_56 Train loss: 0.611126184463501 at step: 151 lr 0.0001\n",
      "2024_05_18_19_09_06 Train loss: 0.23032262921333313 at step: 152 lr 0.0001\n",
      "2024_05_18_19_09_19 Train loss: 0.36098432540893555 at step: 153 lr 0.0001\n",
      "2024_05_18_19_09_30 Train loss: 0.333296537399292 at step: 154 lr 0.0001\n",
      "2024_05_18_19_09_32 Train loss: 0.06878410279750824 at step: 155 lr 0.0001\n",
      "(Val @ epoch 4) acc: 0.9217391304347826; ap: 0.9895915209167894\n",
      "2024_05_18_19_10_00 Train loss: 0.22457176446914673 at step: 156 lr 0.0001\n",
      "2024_05_18_19_10_11 Train loss: 0.1886427402496338 at step: 157 lr 0.0001\n",
      "2024_05_18_19_10_22 Train loss: 0.2339341789484024 at step: 158 lr 0.0001\n",
      "2024_05_18_19_10_31 Train loss: 0.20145711302757263 at step: 159 lr 0.0001\n",
      "2024_05_18_19_10_42 Train loss: 0.20098455250263214 at step: 160 lr 0.0001\n",
      "2024_05_18_19_10_53 Train loss: 0.24670818448066711 at step: 161 lr 0.0001\n",
      "2024_05_18_19_11_06 Train loss: 0.15145237743854523 at step: 162 lr 0.0001\n",
      "2024_05_18_19_11_17 Train loss: 0.10994105041027069 at step: 163 lr 0.0001\n",
      "2024_05_18_19_11_27 Train loss: 0.19786220788955688 at step: 164 lr 0.0001\n",
      "2024_05_18_19_11_35 Train loss: 0.28664565086364746 at step: 165 lr 0.0001\n",
      "2024_05_18_19_11_47 Train loss: 0.14339256286621094 at step: 166 lr 0.0001\n",
      "2024_05_18_19_11_57 Train loss: 0.16478288173675537 at step: 167 lr 0.0001\n",
      "2024_05_18_19_12_05 Train loss: 0.2246691882610321 at step: 168 lr 0.0001\n",
      "2024_05_18_19_12_14 Train loss: 0.14956462383270264 at step: 169 lr 0.0001\n",
      "2024_05_18_19_12_25 Train loss: 0.16775821149349213 at step: 170 lr 0.0001\n",
      "2024_05_18_19_12_37 Train loss: 0.16336122155189514 at step: 171 lr 0.0001\n",
      "2024_05_18_19_12_47 Train loss: 0.10798706114292145 at step: 172 lr 0.0001\n",
      "2024_05_18_19_12_57 Train loss: 0.07854963839054108 at step: 173 lr 0.0001\n",
      "2024_05_18_19_13_06 Train loss: 0.2619536519050598 at step: 174 lr 0.0001\n",
      "2024_05_18_19_13_17 Train loss: 0.08095469325780869 at step: 175 lr 0.0001\n",
      "2024_05_18_19_13_27 Train loss: 0.22032374143600464 at step: 176 lr 0.0001\n",
      "2024_05_18_19_13_34 Train loss: 0.11781710386276245 at step: 177 lr 0.0001\n",
      "2024_05_18_19_13_45 Train loss: 0.1874629259109497 at step: 178 lr 0.0001\n",
      "2024_05_18_19_13_54 Train loss: 0.11445644497871399 at step: 179 lr 0.0001\n",
      "2024_05_18_19_14_02 Train loss: 0.08537977188825607 at step: 180 lr 0.0001\n",
      "2024_05_18_19_14_25 Train loss: 0.23029184341430664 at step: 181 lr 0.0001\n",
      "2024_05_18_19_14_54 Train loss: 0.2873431444168091 at step: 182 lr 0.0001\n",
      "2024_05_18_19_15_15 Train loss: 0.26275572180747986 at step: 183 lr 0.0001\n",
      "2024_05_18_19_15_25 Train loss: 0.11454452574253082 at step: 184 lr 0.0001\n",
      "2024_05_18_19_15_34 Train loss: 0.12142354249954224 at step: 185 lr 0.0001\n",
      "2024_05_18_19_15_34 Train loss: 0.0822814553976059 at step: 186 lr 0.0001\n",
      "(Val @ epoch 5) acc: 0.9304347826086956; ap: 0.9842473050762648\n",
      "2024_05_18_19_16_08 Train loss: 0.16363681852817535 at step: 187 lr 0.0001\n",
      "2024_05_18_19_16_17 Train loss: 0.17018339037895203 at step: 188 lr 0.0001\n",
      "2024_05_18_19_16_24 Train loss: 0.20009750127792358 at step: 189 lr 0.0001\n",
      "2024_05_18_19_16_34 Train loss: 0.06541579961776733 at step: 190 lr 0.0001\n",
      "2024_05_18_19_16_43 Train loss: 0.1037987545132637 at step: 191 lr 0.0001\n",
      "2024_05_18_19_16_52 Train loss: 0.1738022416830063 at step: 192 lr 0.0001\n",
      "2024_05_18_19_17_05 Train loss: 0.23020727932453156 at step: 193 lr 0.0001\n",
      "2024_05_18_19_17_14 Train loss: 0.13406483829021454 at step: 194 lr 0.0001\n",
      "2024_05_18_19_17_25 Train loss: 0.133375883102417 at step: 195 lr 0.0001\n",
      "2024_05_18_19_17_36 Train loss: 0.11639107763767242 at step: 196 lr 0.0001\n",
      "2024_05_18_19_17_43 Train loss: 0.051081158220767975 at step: 197 lr 0.0001\n",
      "2024_05_18_19_17_54 Train loss: 0.20128034055233002 at step: 198 lr 0.0001\n",
      "2024_05_18_19_18_04 Train loss: 0.15035048127174377 at step: 199 lr 0.0001\n",
      "2024_05_18_19_18_14 Train loss: 0.16576479375362396 at step: 200 lr 0.0001\n",
      "2024_05_18_19_18_21 Train loss: 0.3239341378211975 at step: 201 lr 0.0001\n",
      "2024_05_18_19_18_32 Train loss: 0.23404350876808167 at step: 202 lr 0.0001\n",
      "2024_05_18_19_18_43 Train loss: 0.17874082922935486 at step: 203 lr 0.0001\n",
      "2024_05_18_19_18_54 Train loss: 0.14737746119499207 at step: 204 lr 0.0001\n",
      "2024_05_18_19_19_04 Train loss: 0.06980644166469574 at step: 205 lr 0.0001\n",
      "2024_05_18_19_19_13 Train loss: 0.0688244104385376 at step: 206 lr 0.0001\n",
      "2024_05_18_19_19_20 Train loss: 0.03996320068836212 at step: 207 lr 0.0001\n",
      "2024_05_18_19_19_29 Train loss: 0.3130221962928772 at step: 208 lr 0.0001\n",
      "2024_05_18_19_19_40 Train loss: 0.12517371773719788 at step: 209 lr 0.0001\n",
      "2024_05_18_19_19_49 Train loss: 0.2823655903339386 at step: 210 lr 0.0001\n",
      "2024_05_18_19_19_59 Train loss: 0.06681008636951447 at step: 211 lr 0.0001\n",
      "2024_05_18_19_20_08 Train loss: 0.1974077969789505 at step: 212 lr 0.0001\n",
      "2024_05_18_19_20_20 Train loss: 0.3532804548740387 at step: 213 lr 0.0001\n",
      "2024_05_18_19_20_27 Train loss: 0.3529205918312073 at step: 214 lr 0.0001\n",
      "2024_05_18_19_20_34 Train loss: 0.135408416390419 at step: 215 lr 0.0001\n",
      "2024_05_18_19_20_41 Train loss: 0.36403170228004456 at step: 216 lr 0.0001\n",
      "2024_05_18_19_20_42 Train loss: 0.18319284915924072 at step: 217 lr 0.0001\n",
      "(Val @ epoch 6) acc: 0.8173913043478261; ap: 0.9779082952284012\n",
      "2024_05_18_19_21_13 Train loss: 0.19227096438407898 at step: 218 lr 0.0001\n",
      "2024_05_18_19_21_21 Train loss: 0.1910928338766098 at step: 219 lr 0.0001\n",
      "2024_05_18_19_21_30 Train loss: 0.08335632085800171 at step: 220 lr 0.0001\n",
      "2024_05_18_19_21_42 Train loss: 0.5118350982666016 at step: 221 lr 0.0001\n",
      "2024_05_18_19_21_51 Train loss: 0.24539312720298767 at step: 222 lr 0.0001\n",
      "2024_05_18_19_22_02 Train loss: 0.1555950939655304 at step: 223 lr 0.0001\n",
      "2024_05_18_19_22_09 Train loss: 0.2533826231956482 at step: 224 lr 0.0001\n",
      "2024_05_18_19_22_17 Train loss: 0.30099183320999146 at step: 225 lr 0.0001\n",
      "2024_05_18_19_22_27 Train loss: 0.18965142965316772 at step: 226 lr 0.0001\n",
      "2024_05_18_19_22_38 Train loss: 0.33334091305732727 at step: 227 lr 0.0001\n",
      "2024_05_18_19_22_46 Train loss: 0.10529223829507828 at step: 228 lr 0.0001\n",
      "2024_05_18_19_22_56 Train loss: 0.09380929172039032 at step: 229 lr 0.0001\n",
      "2024_05_18_19_23_03 Train loss: 0.0360981747508049 at step: 230 lr 0.0001\n",
      "2024_05_18_19_23_13 Train loss: 0.1438169777393341 at step: 231 lr 0.0001\n",
      "2024_05_18_19_23_23 Train loss: 0.2987063527107239 at step: 232 lr 0.0001\n",
      "2024_05_18_19_23_30 Train loss: 0.16667696833610535 at step: 233 lr 0.0001\n",
      "2024_05_18_19_23_38 Train loss: 0.1721569150686264 at step: 234 lr 0.0001\n",
      "2024_05_18_19_23_50 Train loss: 0.10359840840101242 at step: 235 lr 0.0001\n",
      "2024_05_18_19_24_01 Train loss: 0.10380491614341736 at step: 236 lr 0.0001\n",
      "2024_05_18_19_24_11 Train loss: 0.17798477411270142 at step: 237 lr 0.0001\n",
      "2024_05_18_19_24_20 Train loss: 0.09637309610843658 at step: 238 lr 0.0001\n",
      "2024_05_18_19_24_32 Train loss: 0.20308205485343933 at step: 239 lr 0.0001\n",
      "2024_05_18_19_24_42 Train loss: 0.09996546804904938 at step: 240 lr 0.0001\n",
      "2024_05_18_19_24_50 Train loss: 0.08779139071702957 at step: 241 lr 0.0001\n",
      "2024_05_18_19_25_00 Train loss: 0.054894499480724335 at step: 242 lr 0.0001\n",
      "2024_05_18_19_25_11 Train loss: 0.3834240138530731 at step: 243 lr 0.0001\n",
      "2024_05_18_19_25_25 Train loss: 0.08808548748493195 at step: 244 lr 0.0001\n",
      "2024_05_18_19_25_36 Train loss: 0.25773751735687256 at step: 245 lr 0.0001\n",
      "2024_05_18_19_25_44 Train loss: 0.13166002929210663 at step: 246 lr 0.0001\n",
      "2024_05_18_19_25_55 Train loss: 0.053164318203926086 at step: 247 lr 0.0001\n",
      "2024_05_18_19_25_56 Train loss: 0.03887341916561127 at step: 248 lr 0.0001\n",
      "(Val @ epoch 7) acc: 0.9130434782608695; ap: 0.9742841776429217\n",
      "2024_05_18_19_26_29 Train loss: 0.19356948137283325 at step: 249 lr 0.0001\n",
      "2024_05_18_19_26_57 Train loss: 0.18764066696166992 at step: 250 lr 0.0001\n",
      "2024_05_18_19_27_10 Train loss: 0.18404768407344818 at step: 251 lr 0.0001\n",
      "2024_05_18_19_27_25 Train loss: 0.10840839147567749 at step: 252 lr 0.0001\n",
      "2024_05_18_19_27_34 Train loss: 0.0900043398141861 at step: 253 lr 0.0001\n",
      "2024_05_18_19_27_42 Train loss: 0.2947413921356201 at step: 254 lr 0.0001\n",
      "2024_05_18_19_27_52 Train loss: 0.05996500700712204 at step: 255 lr 0.0001\n",
      "2024_05_18_19_28_01 Train loss: 0.09469494223594666 at step: 256 lr 0.0001\n",
      "2024_05_18_19_28_21 Train loss: 0.16438251733779907 at step: 257 lr 0.0001\n",
      "2024_05_18_19_28_41 Train loss: 0.14724062383174896 at step: 258 lr 0.0001\n",
      "2024_05_18_19_28_54 Train loss: 0.078357994556427 at step: 259 lr 0.0001\n",
      "2024_05_18_19_29_10 Train loss: 0.054023947566747665 at step: 260 lr 0.0001\n",
      "2024_05_18_19_29_24 Train loss: 0.1586831510066986 at step: 261 lr 0.0001\n",
      "2024_05_18_19_29_34 Train loss: 0.15295836329460144 at step: 262 lr 0.0001\n",
      "2024_05_18_19_29_43 Train loss: 0.20701338350772858 at step: 263 lr 0.0001\n",
      "2024_05_18_19_29_51 Train loss: 0.11185875535011292 at step: 264 lr 0.0001\n",
      "2024_05_18_19_30_06 Train loss: 0.21389955282211304 at step: 265 lr 0.0001\n",
      "2024_05_18_19_30_16 Train loss: 0.18009129166603088 at step: 266 lr 0.0001\n",
      "2024_05_18_19_30_23 Train loss: 0.3195018172264099 at step: 267 lr 0.0001\n",
      "2024_05_18_19_30_35 Train loss: 0.4192553758621216 at step: 268 lr 0.0001\n",
      "2024_05_18_19_30_44 Train loss: 0.15176568925380707 at step: 269 lr 0.0001\n",
      "2024_05_18_19_30_53 Train loss: 0.1574665606021881 at step: 270 lr 0.0001\n",
      "2024_05_18_19_31_02 Train loss: 0.07936856895685196 at step: 271 lr 0.0001\n",
      "2024_05_18_19_31_10 Train loss: 0.19413095712661743 at step: 272 lr 0.0001\n",
      "2024_05_18_19_31_16 Train loss: 0.257038950920105 at step: 273 lr 0.0001\n",
      "2024_05_18_19_31_27 Train loss: 0.06680148839950562 at step: 274 lr 0.0001\n",
      "2024_05_18_19_31_39 Train loss: 0.296614408493042 at step: 275 lr 0.0001\n",
      "2024_05_18_19_31_50 Train loss: 0.21254181861877441 at step: 276 lr 0.0001\n",
      "2024_05_18_19_32_00 Train loss: 0.12152349948883057 at step: 277 lr 0.0001\n",
      "2024_05_18_19_32_17 Train loss: 0.14173385500907898 at step: 278 lr 0.0001\n",
      "2024_05_18_19_32_19 Train loss: 0.030820658430457115 at step: 279 lr 0.0001\n",
      "(Val @ epoch 8) acc: 0.9304347826086956; ap: 0.9924299055860716\n",
      "2024_05_18_19_32_54 Train loss: 0.10377247631549835 at step: 280 lr 0.0001\n",
      "2024_05_18_19_33_02 Train loss: 0.029623020440340042 at step: 281 lr 0.0001\n",
      "2024_05_18_19_33_12 Train loss: 0.08684992790222168 at step: 282 lr 0.0001\n",
      "2024_05_18_19_33_19 Train loss: 0.23100775480270386 at step: 283 lr 0.0001\n",
      "2024_05_18_19_33_28 Train loss: 0.12835432589054108 at step: 284 lr 0.0001\n",
      "2024_05_18_19_33_38 Train loss: 0.13448885083198547 at step: 285 lr 0.0001\n",
      "2024_05_18_19_33_49 Train loss: 0.029733091592788696 at step: 286 lr 0.0001\n",
      "2024_05_18_19_33_59 Train loss: 0.08009488135576248 at step: 287 lr 0.0001\n",
      "2024_05_18_19_34_08 Train loss: 0.14310018718242645 at step: 288 lr 0.0001\n",
      "2024_05_18_19_34_17 Train loss: 0.06848112493753433 at step: 289 lr 0.0001\n",
      "2024_05_18_19_34_27 Train loss: 0.13359326124191284 at step: 290 lr 0.0001\n",
      "2024_05_18_19_34_37 Train loss: 0.04518524929881096 at step: 291 lr 0.0001\n",
      "2024_05_18_19_34_45 Train loss: 0.09087122231721878 at step: 292 lr 0.0001\n",
      "2024_05_18_19_34_55 Train loss: 0.21627572178840637 at step: 293 lr 0.0001\n",
      "2024_05_18_19_35_05 Train loss: 0.1345120370388031 at step: 294 lr 0.0001\n",
      "2024_05_18_19_35_16 Train loss: 0.19646161794662476 at step: 295 lr 0.0001\n",
      "2024_05_18_19_35_29 Train loss: 0.2844456434249878 at step: 296 lr 0.0001\n",
      "2024_05_18_19_35_38 Train loss: 0.1557769626379013 at step: 297 lr 0.0001\n",
      "2024_05_18_19_36_04 Train loss: 0.23819220066070557 at step: 298 lr 0.0001\n",
      "2024_05_18_19_36_26 Train loss: 0.06485345214605331 at step: 299 lr 0.0001\n",
      "2024_05_18_19_36_41 Train loss: 0.14931069314479828 at step: 300 lr 0.0001\n",
      "2024_05_18_19_36_54 Train loss: 0.1985664665699005 at step: 301 lr 0.0001\n",
      "2024_05_18_19_37_07 Train loss: 0.43755465745925903 at step: 302 lr 0.0001\n",
      "2024_05_18_19_37_14 Train loss: 0.24473175406455994 at step: 303 lr 0.0001\n",
      "2024_05_18_19_37_21 Train loss: 0.13327857851982117 at step: 304 lr 0.0001\n",
      "2024_05_18_19_37_32 Train loss: 0.1104784607887268 at step: 305 lr 0.0001\n",
      "2024_05_18_19_37_45 Train loss: 0.14236170053482056 at step: 306 lr 0.0001\n",
      "2024_05_18_19_37_56 Train loss: 0.0938597172498703 at step: 307 lr 0.0001\n",
      "2024_05_18_19_38_05 Train loss: 0.06949827820062637 at step: 308 lr 0.0001\n",
      "2024_05_18_19_38_15 Train loss: 0.03737654909491539 at step: 309 lr 0.0001\n",
      "2024_05_18_19_38_16 Train loss: 0.020135484635829926 at step: 310 lr 0.0001\n",
      "(Val @ epoch 9) acc: 0.7130434782608696; ap: 0.9756584871460298\n",
      "2024_05_18_19_38_48 Train loss: 0.05511059984564781 at step: 311 lr 0.0001\n",
      "2024_05_18_19_38_59 Train loss: 0.1547384262084961 at step: 312 lr 0.0001\n",
      "2024_05_18_19_39_18 Train loss: 0.2773030400276184 at step: 313 lr 0.0001\n",
      "2024_05_18_19_39_32 Train loss: 0.21660616993904114 at step: 314 lr 0.0001\n",
      "2024_05_18_19_39_44 Train loss: 0.1059177964925766 at step: 315 lr 0.0001\n",
      "2024_05_18_19_39_59 Train loss: 0.14611195027828217 at step: 316 lr 0.0001\n",
      "2024_05_18_19_40_18 Train loss: 0.11608360707759857 at step: 317 lr 0.0001\n",
      "2024_05_18_19_40_27 Train loss: 0.11946366727352142 at step: 318 lr 0.0001\n",
      "2024_05_18_19_40_32 Train loss: 0.48545512557029724 at step: 319 lr 0.0001\n",
      "2024_05_18_19_40_40 Train loss: 0.10636020451784134 at step: 320 lr 0.0001\n",
      "2024_05_18_19_40_46 Train loss: 0.18502703309059143 at step: 321 lr 0.0001\n",
      "2024_05_18_19_40_53 Train loss: 0.10275284945964813 at step: 322 lr 0.0001\n",
      "2024_05_18_19_41_03 Train loss: 0.10935622453689575 at step: 323 lr 0.0001\n",
      "2024_05_18_19_41_14 Train loss: 0.12373735755681992 at step: 324 lr 0.0001\n",
      "2024_05_18_19_41_24 Train loss: 0.10771061480045319 at step: 325 lr 0.0001\n",
      "2024_05_18_19_41_34 Train loss: 0.12021075934171677 at step: 326 lr 0.0001\n",
      "2024_05_18_19_41_44 Train loss: 0.02190777286887169 at step: 327 lr 0.0001\n",
      "2024_05_18_19_41_54 Train loss: 0.16695675253868103 at step: 328 lr 0.0001\n",
      "2024_05_18_19_42_01 Train loss: 0.6323632001876831 at step: 329 lr 0.0001\n",
      "2024_05_18_19_42_11 Train loss: 0.2186865508556366 at step: 330 lr 0.0001\n",
      "2024_05_18_19_42_20 Train loss: 0.06767697632312775 at step: 331 lr 0.0001\n",
      "2024_05_18_19_42_30 Train loss: 0.09416469931602478 at step: 332 lr 0.0001\n",
      "2024_05_18_19_42_38 Train loss: 0.3037150502204895 at step: 333 lr 0.0001\n",
      "2024_05_18_19_42_48 Train loss: 0.07422074675559998 at step: 334 lr 0.0001\n",
      "2024_05_18_19_42_56 Train loss: 0.08647052943706512 at step: 335 lr 0.0001\n",
      "2024_05_18_19_43_04 Train loss: 0.14646247029304504 at step: 336 lr 0.0001\n",
      "2024_05_18_19_43_14 Train loss: 0.34146764874458313 at step: 337 lr 0.0001\n",
      "2024_05_18_19_43_27 Train loss: 0.4714984595775604 at step: 338 lr 0.0001\n",
      "2024_05_18_19_43_40 Train loss: 0.34777015447616577 at step: 339 lr 0.0001\n",
      "2024_05_18_19_43_51 Train loss: 0.15097525715827942 at step: 340 lr 0.0001\n",
      "2024_05_18_19_43_53 Train loss: 0.027470052242279053 at step: 341 lr 0.0001\n",
      "(Val @ epoch 10) acc: 0.9478260869565217; ap: 0.9882136253488093\n",
      "Saving model ./checkpoints/experiment_name2024_05_18_18_41_03/model_epoch_best.pth\n",
      "2024_05_18_19_44_35 Train loss: 0.1538349986076355 at step: 342 lr 0.0001\n",
      "2024_05_18_19_44_43 Train loss: 0.07253412902355194 at step: 343 lr 0.0001\n",
      "2024_05_18_19_44_50 Train loss: 0.20458151400089264 at step: 344 lr 0.0001\n",
      "2024_05_18_19_45_01 Train loss: 0.3444851040840149 at step: 345 lr 0.0001\n",
      "2024_05_18_19_45_10 Train loss: 0.11242283880710602 at step: 346 lr 0.0001\n",
      "2024_05_18_19_45_20 Train loss: 0.04874596744775772 at step: 347 lr 0.0001\n",
      "2024_05_18_19_45_30 Train loss: 0.19289302825927734 at step: 348 lr 0.0001\n",
      "2024_05_18_19_45_37 Train loss: 0.08604299277067184 at step: 349 lr 0.0001\n",
      "2024_05_18_19_45_49 Train loss: 0.2797461748123169 at step: 350 lr 0.0001\n",
      "2024_05_18_19_45_58 Train loss: 0.2869873046875 at step: 351 lr 0.0001\n",
      "2024_05_18_19_46_07 Train loss: 0.11040443181991577 at step: 352 lr 0.0001\n",
      "2024_05_18_19_46_18 Train loss: 0.21518315374851227 at step: 353 lr 0.0001\n",
      "2024_05_18_19_46_26 Train loss: 0.0923234075307846 at step: 354 lr 0.0001\n",
      "2024_05_18_19_46_38 Train loss: 0.18075966835021973 at step: 355 lr 0.0001\n",
      "2024_05_18_19_46_46 Train loss: 0.16696932911872864 at step: 356 lr 0.0001\n",
      "2024_05_18_19_46_58 Train loss: 0.08053131401538849 at step: 357 lr 0.0001\n",
      "2024_05_18_19_47_06 Train loss: 0.07238262891769409 at step: 358 lr 0.0001\n",
      "2024_05_18_19_47_15 Train loss: 0.07255169749259949 at step: 359 lr 0.0001\n",
      "2024_05_18_19_47_23 Train loss: 0.05205605924129486 at step: 360 lr 0.0001\n",
      "2024_05_18_19_47_31 Train loss: 0.13978499174118042 at step: 361 lr 0.0001\n",
      "2024_05_18_19_47_38 Train loss: 0.2262512743473053 at step: 362 lr 0.0001\n",
      "2024_05_18_19_47_48 Train loss: 0.2864820957183838 at step: 363 lr 0.0001\n",
      "2024_05_18_19_47_58 Train loss: 0.16109782457351685 at step: 364 lr 0.0001\n",
      "2024_05_18_19_48_09 Train loss: 0.075137659907341 at step: 365 lr 0.0001\n",
      "2024_05_18_19_48_19 Train loss: 0.10539615899324417 at step: 366 lr 0.0001\n",
      "2024_05_18_19_48_28 Train loss: 0.059333477169275284 at step: 367 lr 0.0001\n",
      "2024_05_18_19_48_34 Train loss: 0.2345941811800003 at step: 368 lr 0.0001\n",
      "2024_05_18_19_48_43 Train loss: 0.05098402872681618 at step: 369 lr 0.0001\n",
      "2024_05_18_19_48_51 Train loss: 0.2340845912694931 at step: 370 lr 0.0001\n",
      "2024_05_18_19_48_59 Train loss: 0.161124125123024 at step: 371 lr 0.0001\n",
      "2024_05_18_19_49_00 Train loss: 0.034767117351293564 at step: 372 lr 0.0001\n",
      "(Val @ epoch 11) acc: 0.9478260869565217; ap: 0.9977808417551269\n",
      "2024_05_18_19_49_33 Train loss: 0.06766828894615173 at step: 373 lr 0.0001\n",
      "2024_05_18_19_49_43 Train loss: 0.06788349151611328 at step: 374 lr 0.0001\n",
      "2024_05_18_19_49_51 Train loss: 0.05923883616924286 at step: 375 lr 0.0001\n",
      "2024_05_18_19_50_00 Train loss: 0.04521387070417404 at step: 376 lr 0.0001\n",
      "2024_05_18_19_50_05 Train loss: 0.1806824803352356 at step: 377 lr 0.0001\n",
      "2024_05_18_19_50_16 Train loss: 0.11087647080421448 at step: 378 lr 0.0001\n",
      "2024_05_18_19_50_27 Train loss: 0.11370711028575897 at step: 379 lr 0.0001\n",
      "2024_05_18_19_50_48 Train loss: 0.03756161034107208 at step: 380 lr 0.0001\n",
      "2024_05_18_19_51_10 Train loss: 0.24834923446178436 at step: 381 lr 0.0001\n",
      "2024_05_18_19_51_33 Train loss: 0.028987688943743706 at step: 382 lr 0.0001\n",
      "2024_05_18_19_51_42 Train loss: 0.060678549110889435 at step: 383 lr 0.0001\n",
      "2024_05_18_19_51_52 Train loss: 0.06713438034057617 at step: 384 lr 0.0001\n",
      "2024_05_18_19_52_03 Train loss: 0.046969860792160034 at step: 385 lr 0.0001\n",
      "2024_05_18_19_52_14 Train loss: 0.10143926739692688 at step: 386 lr 0.0001\n",
      "2024_05_18_19_52_24 Train loss: 0.055964790284633636 at step: 387 lr 0.0001\n",
      "2024_05_18_19_52_30 Train loss: 0.2382344752550125 at step: 388 lr 0.0001\n",
      "2024_05_18_19_52_42 Train loss: 0.07368047535419464 at step: 389 lr 0.0001\n",
      "2024_05_18_19_52_51 Train loss: 0.10242293775081635 at step: 390 lr 0.0001\n",
      "2024_05_18_19_53_01 Train loss: 0.0917605459690094 at step: 391 lr 0.0001\n",
      "2024_05_18_19_53_13 Train loss: 0.1641407310962677 at step: 392 lr 0.0001\n",
      "2024_05_18_19_53_22 Train loss: 0.0631130188703537 at step: 393 lr 0.0001\n",
      "2024_05_18_19_53_33 Train loss: 0.3331618010997772 at step: 394 lr 0.0001\n",
      "2024_05_18_19_53_43 Train loss: 0.15881018340587616 at step: 395 lr 0.0001\n",
      "2024_05_18_19_53_55 Train loss: 0.239657461643219 at step: 396 lr 0.0001\n",
      "2024_05_18_19_54_05 Train loss: 0.3082037568092346 at step: 397 lr 0.0001\n",
      "2024_05_18_19_54_17 Train loss: 0.25774842500686646 at step: 398 lr 0.0001\n",
      "2024_05_18_19_54_24 Train loss: 0.09785068780183792 at step: 399 lr 0.0001\n",
      "2024_05_18_19_54_34 Train loss: 0.18023207783699036 at step: 400 lr 0.0001\n",
      "2024_05_18_19_54_44 Train loss: 0.20038846135139465 at step: 401 lr 0.0001\n",
      "2024_05_18_19_54_50 Train loss: 0.24030086398124695 at step: 402 lr 0.0001\n",
      "2024_05_18_19_54_51 Train loss: 0.34861111640930176 at step: 403 lr 0.0001\n",
      "(Val @ epoch 12) acc: 0.9304347826086956; ap: 0.9793908489918649\n",
      "2024_05_18_19_55_32 Train loss: 0.11787642538547516 at step: 404 lr 0.0001\n",
      "2024_05_18_19_55_41 Train loss: 0.04009716212749481 at step: 405 lr 0.0001\n",
      "2024_05_18_19_55_51 Train loss: 0.05714036151766777 at step: 406 lr 0.0001\n",
      "2024_05_18_19_55_58 Train loss: 0.086433544754982 at step: 407 lr 0.0001\n",
      "2024_05_18_19_56_08 Train loss: 0.10720103979110718 at step: 408 lr 0.0001\n",
      "2024_05_18_19_56_18 Train loss: 0.08172782510519028 at step: 409 lr 0.0001\n",
      "2024_05_18_19_56_26 Train loss: 0.11775729805231094 at step: 410 lr 0.0001\n",
      "2024_05_18_19_56_37 Train loss: 0.20631270110607147 at step: 411 lr 0.0001\n",
      "2024_05_18_19_56_49 Train loss: 0.10377383232116699 at step: 412 lr 0.0001\n",
      "2024_05_18_19_57_01 Train loss: 0.46267521381378174 at step: 413 lr 0.0001\n",
      "2024_05_18_19_57_08 Train loss: 0.10759502649307251 at step: 414 lr 0.0001\n",
      "2024_05_18_19_57_18 Train loss: 0.19916093349456787 at step: 415 lr 0.0001\n",
      "2024_05_18_19_57_27 Train loss: 0.11591614782810211 at step: 416 lr 0.0001\n",
      "2024_05_18_19_57_52 Train loss: 0.3497360348701477 at step: 417 lr 0.0001\n",
      "2024_05_18_19_57_58 Train loss: 0.17653727531433105 at step: 418 lr 0.0001\n",
      "2024_05_18_19_58_08 Train loss: 0.15658751130104065 at step: 419 lr 0.0001\n",
      "2024_05_18_19_58_14 Train loss: 0.30938416719436646 at step: 420 lr 0.0001\n",
      "2024_05_18_19_58_23 Train loss: 0.20695644617080688 at step: 421 lr 0.0001\n",
      "2024_05_18_19_58_31 Train loss: 0.3412361443042755 at step: 422 lr 0.0001\n",
      "2024_05_18_19_58_40 Train loss: 0.06812778115272522 at step: 423 lr 0.0001\n",
      "2024_05_18_19_58_50 Train loss: 0.28292250633239746 at step: 424 lr 0.0001\n",
      "2024_05_18_19_59_00 Train loss: 0.11600317060947418 at step: 425 lr 0.0001\n",
      "2024_05_18_19_59_09 Train loss: 0.13172805309295654 at step: 426 lr 0.0001\n",
      "2024_05_18_19_59_15 Train loss: 0.49823880195617676 at step: 427 lr 0.0001\n",
      "2024_05_18_19_59_26 Train loss: 0.1201343685388565 at step: 428 lr 0.0001\n",
      "2024_05_18_19_59_36 Train loss: 0.0604960173368454 at step: 429 lr 0.0001\n",
      "2024_05_18_19_59_43 Train loss: 0.12652245163917542 at step: 430 lr 0.0001\n",
      "2024_05_18_19_59_51 Train loss: 0.16780242323875427 at step: 431 lr 0.0001\n",
      "2024_05_18_20_00_02 Train loss: 0.11549971997737885 at step: 432 lr 0.0001\n",
      "2024_05_18_20_00_13 Train loss: 0.15769849717617035 at step: 433 lr 0.0001\n",
      "2024_05_18_20_00_15 Train loss: 1.2920143604278564 at step: 434 lr 0.0001\n",
      "(Val @ epoch 13) acc: 0.9478260869565217; ap: 0.996483583686886\n",
      "2024_05_18_20_00_47 Train loss: 0.11695016920566559 at step: 435 lr 0.0001\n",
      "2024_05_18_20_01_00 Train loss: 0.14393562078475952 at step: 436 lr 0.0001\n",
      "2024_05_18_20_01_10 Train loss: 0.018282966688275337 at step: 437 lr 0.0001\n",
      "2024_05_18_20_01_20 Train loss: 0.3059651255607605 at step: 438 lr 0.0001\n",
      "2024_05_18_20_01_30 Train loss: 0.029674556106328964 at step: 439 lr 0.0001\n",
      "2024_05_18_20_01_40 Train loss: 0.0364808514714241 at step: 440 lr 0.0001\n",
      "2024_05_18_20_01_50 Train loss: 0.18880042433738708 at step: 441 lr 0.0001\n",
      "2024_05_18_20_02_00 Train loss: 0.10410460829734802 at step: 442 lr 0.0001\n",
      "2024_05_18_20_02_12 Train loss: 0.12798011302947998 at step: 443 lr 0.0001\n",
      "2024_05_18_20_02_24 Train loss: 0.06930935382843018 at step: 444 lr 0.0001\n",
      "2024_05_18_20_02_34 Train loss: 0.057834871113300323 at step: 445 lr 0.0001\n",
      "2024_05_18_20_02_42 Train loss: 0.24465809762477875 at step: 446 lr 0.0001\n",
      "2024_05_18_20_02_50 Train loss: 0.23208390176296234 at step: 447 lr 0.0001\n",
      "2024_05_18_20_03_01 Train loss: 0.07676137983798981 at step: 448 lr 0.0001\n",
      "2024_05_18_20_03_10 Train loss: 0.10050314664840698 at step: 449 lr 0.0001\n",
      "2024_05_18_20_03_22 Train loss: 0.06316065788269043 at step: 450 lr 0.0001\n",
      "2024_05_18_20_03_32 Train loss: 0.20512987673282623 at step: 451 lr 0.0001\n",
      "2024_05_18_20_03_43 Train loss: 0.18919788300991058 at step: 452 lr 0.0001\n",
      "2024_05_18_20_03_51 Train loss: 0.15999606251716614 at step: 453 lr 0.0001\n",
      "2024_05_18_20_04_03 Train loss: 0.13185496628284454 at step: 454 lr 0.0001\n",
      "2024_05_18_20_04_11 Train loss: 0.07324165105819702 at step: 455 lr 0.0001\n",
      "2024_05_18_20_04_21 Train loss: 0.04126671329140663 at step: 456 lr 0.0001\n",
      "2024_05_18_20_04_29 Train loss: 0.10731381177902222 at step: 457 lr 0.0001\n",
      "2024_05_18_20_04_37 Train loss: 0.06803767383098602 at step: 458 lr 0.0001\n",
      "2024_05_18_20_04_48 Train loss: 0.12392318248748779 at step: 459 lr 0.0001\n",
      "2024_05_18_20_04_54 Train loss: 0.15892289578914642 at step: 460 lr 0.0001\n",
      "2024_05_18_20_05_03 Train loss: 0.05917222797870636 at step: 461 lr 0.0001\n",
      "2024_05_18_20_05_10 Train loss: 0.09859439730644226 at step: 462 lr 0.0001\n",
      "2024_05_18_20_05_21 Train loss: 0.04746311157941818 at step: 463 lr 0.0001\n",
      "2024_05_18_20_05_34 Train loss: 0.23879212141036987 at step: 464 lr 0.0001\n",
      "2024_05_18_20_05_37 Train loss: 1.2492802143096924 at step: 465 lr 0.0001\n",
      "(Val @ epoch 14) acc: 0.9130434782608695; ap: 0.9967781816383553\n",
      "2024_05_18_20_06_10 Train loss: 0.022478654980659485 at step: 466 lr 0.0001\n",
      "2024_05_18_20_06_18 Train loss: 0.1588159203529358 at step: 467 lr 0.0001\n",
      "2024_05_18_20_06_28 Train loss: 0.37564724683761597 at step: 468 lr 0.0001\n",
      "2024_05_18_20_06_35 Train loss: 0.321122407913208 at step: 469 lr 0.0001\n",
      "2024_05_18_20_06_45 Train loss: 0.25422337651252747 at step: 470 lr 0.0001\n",
      "2024_05_18_20_06_54 Train loss: 0.17889761924743652 at step: 471 lr 0.0001\n",
      "2024_05_18_20_07_02 Train loss: 0.1857350468635559 at step: 472 lr 0.0001\n",
      "2024_05_18_20_07_14 Train loss: 0.05315563827753067 at step: 473 lr 0.0001\n",
      "2024_05_18_20_07_21 Train loss: 0.13132545351982117 at step: 474 lr 0.0001\n",
      "2024_05_18_20_07_29 Train loss: 0.09967900812625885 at step: 475 lr 0.0001\n",
      "2024_05_18_20_07_37 Train loss: 0.14694133400917053 at step: 476 lr 0.0001\n",
      "2024_05_18_20_07_52 Train loss: 0.18463778495788574 at step: 477 lr 0.0001\n",
      "2024_05_18_20_08_02 Train loss: 0.07854165136814117 at step: 478 lr 0.0001\n",
      "2024_05_18_20_08_13 Train loss: 0.2875673770904541 at step: 479 lr 0.0001\n",
      "2024_05_18_20_08_19 Train loss: 0.4467197060585022 at step: 480 lr 0.0001\n",
      "2024_05_18_20_08_29 Train loss: 0.1917809247970581 at step: 481 lr 0.0001\n",
      "2024_05_18_20_08_38 Train loss: 0.17366598546504974 at step: 482 lr 0.0001\n",
      "2024_05_18_20_08_49 Train loss: 0.16703179478645325 at step: 483 lr 0.0001\n",
      "2024_05_18_20_08_58 Train loss: 0.11213850975036621 at step: 484 lr 0.0001\n",
      "2024_05_18_20_09_09 Train loss: 0.12024770677089691 at step: 485 lr 0.0001\n",
      "2024_05_18_20_09_16 Train loss: 0.1007375717163086 at step: 486 lr 0.0001\n",
      "2024_05_18_20_09_27 Train loss: 0.08131925761699677 at step: 487 lr 0.0001\n",
      "2024_05_18_20_09_37 Train loss: 0.09190583974123001 at step: 488 lr 0.0001\n",
      "2024_05_18_20_09_44 Train loss: 0.11295227706432343 at step: 489 lr 0.0001\n",
      "2024_05_18_20_09_57 Train loss: 0.2342340499162674 at step: 490 lr 0.0001\n",
      "2024_05_18_20_10_06 Train loss: 0.07781952619552612 at step: 491 lr 0.0001\n",
      "2024_05_18_20_10_16 Train loss: 0.15657103061676025 at step: 492 lr 0.0001\n",
      "2024_05_18_20_10_29 Train loss: 0.08150561898946762 at step: 493 lr 0.0001\n",
      "2024_05_18_20_10_38 Train loss: 0.08985040336847305 at step: 494 lr 0.0001\n",
      "2024_05_18_20_10_46 Train loss: 0.14276659488677979 at step: 495 lr 0.0001\n",
      "2024_05_18_20_10_48 Train loss: 0.02484242618083954 at step: 496 lr 0.0001\n",
      "(Val @ epoch 15) acc: 0.7913043478260869; ap: 0.9737701293586584\n",
      "2024_05_18_20_11_25 Train loss: 0.17104507982730865 at step: 497 lr 0.0001\n",
      "2024_05_18_20_11_36 Train loss: 0.12026675045490265 at step: 498 lr 0.0001\n",
      "2024_05_18_20_11_45 Train loss: 0.11897540092468262 at step: 499 lr 0.0001\n",
      "2024_05_18_20_11_52 Train loss: 0.1821514219045639 at step: 500 lr 0.0001\n",
      "2024_05_18_20_12_02 Train loss: 0.04204677790403366 at step: 501 lr 0.0001\n",
      "2024_05_18_20_12_07 Train loss: 0.15533827245235443 at step: 502 lr 0.0001\n",
      "2024_05_18_20_12_14 Train loss: 0.13828769326210022 at step: 503 lr 0.0001\n",
      "2024_05_18_20_12_23 Train loss: 0.13138549029827118 at step: 504 lr 0.0001\n",
      "2024_05_18_20_12_36 Train loss: 0.18758194148540497 at step: 505 lr 0.0001\n",
      "2024_05_18_20_12_44 Train loss: 0.1724085807800293 at step: 506 lr 0.0001\n",
      "2024_05_18_20_12_52 Train loss: 0.14668264985084534 at step: 507 lr 0.0001\n",
      "2024_05_18_20_13_00 Train loss: 0.16856952011585236 at step: 508 lr 0.0001\n",
      "2024_05_18_20_13_13 Train loss: 0.2325741946697235 at step: 509 lr 0.0001\n",
      "2024_05_18_20_13_22 Train loss: 0.056229118257761 at step: 510 lr 0.0001\n",
      "2024_05_18_20_13_33 Train loss: 0.09244606643915176 at step: 511 lr 0.0001\n",
      "2024_05_18_20_13_46 Train loss: 0.1914893537759781 at step: 512 lr 0.0001\n",
      "2024_05_18_20_13_55 Train loss: 0.22524046897888184 at step: 513 lr 0.0001\n",
      "2024_05_18_20_14_04 Train loss: 0.08598184585571289 at step: 514 lr 0.0001\n",
      "2024_05_18_20_14_11 Train loss: 0.04614049196243286 at step: 515 lr 0.0001\n",
      "2024_05_18_20_14_23 Train loss: 0.05145677924156189 at step: 516 lr 0.0001\n",
      "2024_05_18_20_14_30 Train loss: 0.06557215750217438 at step: 517 lr 0.0001\n",
      "2024_05_18_20_14_40 Train loss: 0.09867404401302338 at step: 518 lr 0.0001\n",
      "2024_05_18_20_14_46 Train loss: 0.1121879294514656 at step: 519 lr 0.0001\n",
      "2024_05_18_20_14_59 Train loss: 0.3568546175956726 at step: 520 lr 0.0001\n",
      "2024_05_18_20_15_11 Train loss: 0.24191591143608093 at step: 521 lr 0.0001\n",
      "2024_05_18_20_15_23 Train loss: 0.03428833931684494 at step: 522 lr 0.0001\n",
      "2024_05_18_20_15_47 Train loss: 0.07707297801971436 at step: 523 lr 0.0001\n",
      "2024_05_18_20_16_05 Train loss: 0.06599584966897964 at step: 524 lr 0.0001\n",
      "2024_05_18_20_16_21 Train loss: 0.11013734340667725 at step: 525 lr 0.0001\n",
      "2024_05_18_20_16_36 Train loss: 0.047215450555086136 at step: 526 lr 0.0001\n",
      "2024_05_18_20_16_37 Train loss: 0.16730326414108276 at step: 527 lr 0.0001\n",
      "(Val @ epoch 16) acc: 0.8521739130434782; ap: 0.9947872484834898\n",
      "2024_05_18_20_17_08 Train loss: 0.22094126045703888 at step: 528 lr 0.0001\n",
      "2024_05_18_20_17_21 Train loss: 0.21123655140399933 at step: 529 lr 0.0001\n",
      "2024_05_18_20_17_29 Train loss: 0.09141954779624939 at step: 530 lr 0.0001\n",
      "2024_05_18_20_17_40 Train loss: 0.055796489119529724 at step: 531 lr 0.0001\n",
      "2024_05_18_20_17_53 Train loss: 0.2683693766593933 at step: 532 lr 0.0001\n",
      "2024_05_18_20_18_05 Train loss: 0.19100525975227356 at step: 533 lr 0.0001\n",
      "2024_05_18_20_18_15 Train loss: 0.03318311274051666 at step: 534 lr 0.0001\n",
      "2024_05_18_20_18_22 Train loss: 0.17503084242343903 at step: 535 lr 0.0001\n",
      "2024_05_18_20_18_29 Train loss: 0.18066199123859406 at step: 536 lr 0.0001\n",
      "2024_05_18_20_18_40 Train loss: 0.08195745944976807 at step: 537 lr 0.0001\n",
      "2024_05_18_20_18_49 Train loss: 0.11162800341844559 at step: 538 lr 0.0001\n",
      "2024_05_18_20_18_57 Train loss: 0.09864561259746552 at step: 539 lr 0.0001\n",
      "2024_05_18_20_19_03 Train loss: 0.28237760066986084 at step: 540 lr 0.0001\n",
      "2024_05_18_20_19_12 Train loss: 0.1348208487033844 at step: 541 lr 0.0001\n",
      "2024_05_18_20_19_19 Train loss: 0.04825446754693985 at step: 542 lr 0.0001\n",
      "2024_05_18_20_19_29 Train loss: 0.16527359187602997 at step: 543 lr 0.0001\n",
      "2024_05_18_20_19_39 Train loss: 0.07251730561256409 at step: 544 lr 0.0001\n",
      "2024_05_18_20_19_47 Train loss: 0.22731740772724152 at step: 545 lr 0.0001\n",
      "2024_05_18_20_19_59 Train loss: 0.17389902472496033 at step: 546 lr 0.0001\n",
      "2024_05_18_20_20_12 Train loss: 0.11214839667081833 at step: 547 lr 0.0001\n",
      "2024_05_18_20_20_21 Train loss: 0.08685103803873062 at step: 548 lr 0.0001\n",
      "2024_05_18_20_20_30 Train loss: 0.08904892951250076 at step: 549 lr 0.0001\n",
      "2024_05_18_20_20_39 Train loss: 0.16213062405586243 at step: 550 lr 0.0001\n",
      "2024_05_18_20_20_47 Train loss: 0.1224856823682785 at step: 551 lr 0.0001\n",
      "2024_05_18_20_20_55 Train loss: 0.1955692172050476 at step: 552 lr 0.0001\n",
      "2024_05_18_20_21_06 Train loss: 0.12248238921165466 at step: 553 lr 0.0001\n",
      "2024_05_18_20_21_13 Train loss: 0.10017044842243195 at step: 554 lr 0.0001\n",
      "2024_05_18_20_21_22 Train loss: 0.1857367902994156 at step: 555 lr 0.0001\n",
      "2024_05_18_20_21_32 Train loss: 0.25575119256973267 at step: 556 lr 0.0001\n",
      "2024_05_18_20_21_40 Train loss: 0.2989949882030487 at step: 557 lr 0.0001\n",
      "2024_05_18_20_21_40 Train loss: 0.8751976490020752 at step: 558 lr 0.0001\n",
      "(Val @ epoch 17) acc: 0.9304347826086956; ap: 0.9987228607918263\n",
      "2024_05_18_20_22_17 Train loss: 0.23333945870399475 at step: 559 lr 0.0001\n",
      "2024_05_18_20_22_30 Train loss: 0.16217301785945892 at step: 560 lr 0.0001\n",
      "2024_05_18_20_22_37 Train loss: 0.077120840549469 at step: 561 lr 0.0001\n",
      "2024_05_18_20_22_46 Train loss: 0.08101076632738113 at step: 562 lr 0.0001\n",
      "2024_05_18_20_22_56 Train loss: 0.12046514451503754 at step: 563 lr 0.0001\n",
      "2024_05_18_20_23_06 Train loss: 0.14660346508026123 at step: 564 lr 0.0001\n",
      "2024_05_18_20_23_14 Train loss: 0.07027390599250793 at step: 565 lr 0.0001\n",
      "2024_05_18_20_23_23 Train loss: 0.18286287784576416 at step: 566 lr 0.0001\n",
      "2024_05_18_20_23_30 Train loss: 0.1622399240732193 at step: 567 lr 0.0001\n",
      "2024_05_18_20_23_48 Train loss: 0.059801749885082245 at step: 568 lr 0.0001\n",
      "2024_05_18_20_24_08 Train loss: 0.19961026310920715 at step: 569 lr 0.0001\n",
      "2024_05_18_20_24_29 Train loss: 0.26861947774887085 at step: 570 lr 0.0001\n",
      "2024_05_18_20_24_42 Train loss: 0.15950773656368256 at step: 571 lr 0.0001\n",
      "2024_05_18_20_24_51 Train loss: 0.1673658788204193 at step: 572 lr 0.0001\n",
      "2024_05_18_20_25_02 Train loss: 0.040258683264255524 at step: 573 lr 0.0001\n",
      "2024_05_18_20_25_11 Train loss: 0.04714261740446091 at step: 574 lr 0.0001\n",
      "2024_05_18_20_25_22 Train loss: 0.07270374149084091 at step: 575 lr 0.0001\n",
      "2024_05_18_20_25_30 Train loss: 0.1804989129304886 at step: 576 lr 0.0001\n",
      "2024_05_18_20_25_36 Train loss: 0.1091545969247818 at step: 577 lr 0.0001\n",
      "2024_05_18_20_25_41 Train loss: 0.15053611993789673 at step: 578 lr 0.0001\n",
      "2024_05_18_20_25_50 Train loss: 0.14310136437416077 at step: 579 lr 0.0001\n",
      "2024_05_18_20_26_01 Train loss: 0.16586750745773315 at step: 580 lr 0.0001\n",
      "2024_05_18_20_26_12 Train loss: 0.12054863572120667 at step: 581 lr 0.0001\n",
      "2024_05_18_20_26_23 Train loss: 0.18878833949565887 at step: 582 lr 0.0001\n",
      "2024_05_18_20_26_34 Train loss: 0.10607501864433289 at step: 583 lr 0.0001\n",
      "2024_05_18_20_26_42 Train loss: 0.13600528240203857 at step: 584 lr 0.0001\n",
      "2024_05_18_20_26_52 Train loss: 0.042943067848682404 at step: 585 lr 0.0001\n",
      "2024_05_18_20_27_01 Train loss: 0.03069465234875679 at step: 586 lr 0.0001\n",
      "2024_05_18_20_27_13 Train loss: 0.14842775464057922 at step: 587 lr 0.0001\n",
      "2024_05_18_20_27_24 Train loss: 0.11837995052337646 at step: 588 lr 0.0001\n",
      "2024_05_18_20_27_25 Train loss: 0.11555414646863937 at step: 589 lr 0.0001\n",
      "(Val @ epoch 18) acc: 0.9304347826086956; ap: 0.9915240475631677\n",
      "2024_05_18_20_28_01 Train loss: 0.09597159922122955 at step: 590 lr 0.0001\n",
      "2024_05_18_20_28_10 Train loss: 0.05819019675254822 at step: 591 lr 0.0001\n",
      "2024_05_18_20_28_24 Train loss: 0.1774289906024933 at step: 592 lr 0.0001\n",
      "2024_05_18_20_28_34 Train loss: 0.09838183224201202 at step: 593 lr 0.0001\n",
      "2024_05_18_20_28_43 Train loss: 0.048692360520362854 at step: 594 lr 0.0001\n",
      "2024_05_18_20_28_51 Train loss: 0.07538962364196777 at step: 595 lr 0.0001\n",
      "2024_05_18_20_29_03 Train loss: 0.11101369559764862 at step: 596 lr 0.0001\n",
      "2024_05_18_20_29_12 Train loss: 0.07999309152364731 at step: 597 lr 0.0001\n",
      "2024_05_18_20_29_25 Train loss: 0.0586819164454937 at step: 598 lr 0.0001\n",
      "2024_05_18_20_29_37 Train loss: 0.11121175438165665 at step: 599 lr 0.0001\n",
      "2024_05_18_20_29_49 Train loss: 0.03312007710337639 at step: 600 lr 0.0001\n",
      "2024_05_18_20_30_01 Train loss: 0.07966149598360062 at step: 601 lr 0.0001\n",
      "2024_05_18_20_30_11 Train loss: 0.17622222006320953 at step: 602 lr 0.0001\n",
      "2024_05_18_20_30_24 Train loss: 0.15687870979309082 at step: 603 lr 0.0001\n",
      "2024_05_18_20_30_37 Train loss: 0.03350391238927841 at step: 604 lr 0.0001\n",
      "2024_05_18_20_30_58 Train loss: 0.019038110971450806 at step: 605 lr 0.0001\n",
      "2024_05_18_20_31_19 Train loss: 0.09670066833496094 at step: 606 lr 0.0001\n",
      "2024_05_18_20_31_45 Train loss: 0.07933992892503738 at step: 607 lr 0.0001\n",
      "2024_05_18_20_31_53 Train loss: 0.1422041803598404 at step: 608 lr 0.0001\n",
      "2024_05_18_20_32_04 Train loss: 0.05468618869781494 at step: 609 lr 0.0001\n",
      "2024_05_18_20_32_13 Train loss: 0.09368070960044861 at step: 610 lr 0.0001\n",
      "2024_05_18_20_32_22 Train loss: 0.07510662823915482 at step: 611 lr 0.0001\n",
      "2024_05_18_20_32_32 Train loss: 0.042589426040649414 at step: 612 lr 0.0001\n",
      "2024_05_18_20_32_40 Train loss: 0.1478407084941864 at step: 613 lr 0.0001\n",
      "2024_05_18_20_32_46 Train loss: 0.07913072407245636 at step: 614 lr 0.0001\n",
      "2024_05_18_20_32_55 Train loss: 0.06127908080816269 at step: 615 lr 0.0001\n",
      "2024_05_18_20_33_04 Train loss: 0.11240456998348236 at step: 616 lr 0.0001\n",
      "2024_05_18_20_33_17 Train loss: 0.0449095144867897 at step: 617 lr 0.0001\n",
      "2024_05_18_20_33_26 Train loss: 0.02363673411309719 at step: 618 lr 0.0001\n",
      "2024_05_18_20_33_34 Train loss: 0.049664247781038284 at step: 619 lr 0.0001\n",
      "2024_05_18_20_33_34 Train loss: 0.004695860669016838 at step: 620 lr 0.0001\n",
      "(Val @ epoch 19) acc: 0.9043478260869565; ap: 0.9963640835493072\n",
      "2024_05_18_20_34_10 Train loss: 0.042182184755802155 at step: 621 lr 0.0001\n",
      "2024_05_18_20_34_19 Train loss: 0.04412864148616791 at step: 622 lr 0.0001\n",
      "2024_05_18_20_34_29 Train loss: 0.02434222400188446 at step: 623 lr 0.0001\n",
      "2024_05_18_20_34_38 Train loss: 0.10544547438621521 at step: 624 lr 0.0001\n",
      "2024_05_18_20_34_46 Train loss: 0.09136282652616501 at step: 625 lr 0.0001\n",
      "2024_05_18_20_34_59 Train loss: 0.1777045875787735 at step: 626 lr 0.0001\n",
      "2024_05_18_20_35_07 Train loss: 0.051595065742731094 at step: 627 lr 0.0001\n",
      "2024_05_18_20_35_19 Train loss: 0.049568578600883484 at step: 628 lr 0.0001\n",
      "2024_05_18_20_35_30 Train loss: 0.08343655616044998 at step: 629 lr 0.0001\n",
      "2024_05_18_20_35_38 Train loss: 0.08558794856071472 at step: 630 lr 0.0001\n",
      "2024_05_18_20_35_48 Train loss: 0.05298149585723877 at step: 631 lr 0.0001\n",
      "2024_05_18_20_35_57 Train loss: 0.06772368401288986 at step: 632 lr 0.0001\n",
      "2024_05_18_20_36_07 Train loss: 0.06417824327945709 at step: 633 lr 0.0001\n",
      "2024_05_18_20_36_15 Train loss: 0.17635250091552734 at step: 634 lr 0.0001\n",
      "2024_05_18_20_36_24 Train loss: 0.13422583043575287 at step: 635 lr 0.0001\n",
      "2024_05_18_20_36_32 Train loss: 0.11403446644544601 at step: 636 lr 0.0001\n",
      "2024_05_18_20_36_44 Train loss: 0.0537363737821579 at step: 637 lr 0.0001\n",
      "2024_05_18_20_36_52 Train loss: 0.053911030292510986 at step: 638 lr 0.0001\n",
      "2024_05_18_20_37_03 Train loss: 0.11724554747343063 at step: 639 lr 0.0001\n",
      "2024_05_18_20_37_11 Train loss: 0.020808840170502663 at step: 640 lr 0.0001\n",
      "2024_05_18_20_37_19 Train loss: 0.06968840211629868 at step: 641 lr 0.0001\n",
      "2024_05_18_20_37_28 Train loss: 0.03496333956718445 at step: 642 lr 0.0001\n",
      "2024_05_18_20_37_38 Train loss: 0.16260920464992523 at step: 643 lr 0.0001\n",
      "2024_05_18_20_37_44 Train loss: 0.1265554130077362 at step: 644 lr 0.0001\n",
      "2024_05_18_20_37_57 Train loss: 0.19770212471485138 at step: 645 lr 0.0001\n",
      "2024_05_18_20_38_07 Train loss: 0.04121822863817215 at step: 646 lr 0.0001\n",
      "2024_05_18_20_38_16 Train loss: 0.10120763629674911 at step: 647 lr 0.0001\n",
      "2024_05_18_20_38_29 Train loss: 0.182442307472229 at step: 648 lr 0.0001\n",
      "2024_05_18_20_38_36 Train loss: 0.12016062438488007 at step: 649 lr 0.0001\n",
      "2024_05_18_20_38_46 Train loss: 0.05844102054834366 at step: 650 lr 0.0001\n",
      "2024_05_18_20_38_47 Train loss: 0.008945263922214508 at step: 651 lr 0.0001\n",
      "2024_05_18_20_38_47 changing lr at the end of epoch 20, iters 651\n",
      "*************************\n",
      "Changing lr from 0.0001 to 9e-05\n",
      "*************************\n",
      "(Val @ epoch 20) acc: 0.9826086956521739; ap: 0.9910506956786874\n",
      "Saving model ./checkpoints/experiment_name2024_05_18_18_41_03/model_epoch_best.pth\n",
      "2024_05_18_20_39_19 Train loss: 0.04625330865383148 at step: 652 lr 9e-05\n",
      "2024_05_18_20_39_32 Train loss: 0.12398213148117065 at step: 653 lr 9e-05\n",
      "2024_05_18_20_39_39 Train loss: 0.07006354629993439 at step: 654 lr 9e-05\n",
      "2024_05_18_20_39_50 Train loss: 0.046286895871162415 at step: 655 lr 9e-05\n",
      "2024_05_18_20_40_00 Train loss: 0.03727445751428604 at step: 656 lr 9e-05\n",
      "2024_05_18_20_40_08 Train loss: 0.024897431954741478 at step: 657 lr 9e-05\n",
      "2024_05_18_20_40_21 Train loss: 0.044652894139289856 at step: 658 lr 9e-05\n",
      "2024_05_18_20_40_30 Train loss: 0.06538184732198715 at step: 659 lr 9e-05\n",
      "2024_05_18_20_40_39 Train loss: 0.06413963437080383 at step: 660 lr 9e-05\n",
      "2024_05_18_20_40_49 Train loss: 0.026882896199822426 at step: 661 lr 9e-05\n",
      "2024_05_18_20_40_58 Train loss: 0.07375283539295197 at step: 662 lr 9e-05\n",
      "2024_05_18_20_41_09 Train loss: 0.17640578746795654 at step: 663 lr 9e-05\n",
      "2024_05_18_20_41_22 Train loss: 0.15807463228702545 at step: 664 lr 9e-05\n",
      "2024_05_18_20_41_30 Train loss: 0.24180305004119873 at step: 665 lr 9e-05\n",
      "2024_05_18_20_41_39 Train loss: 0.2142052799463272 at step: 666 lr 9e-05\n",
      "2024_05_18_20_41_50 Train loss: 0.08960926532745361 at step: 667 lr 9e-05\n",
      "2024_05_18_20_41_59 Train loss: 0.032411839812994 at step: 668 lr 9e-05\n",
      "2024_05_18_20_42_10 Train loss: 0.02174326963722706 at step: 669 lr 9e-05\n",
      "2024_05_18_20_42_21 Train loss: 0.032048314809799194 at step: 670 lr 9e-05\n",
      "2024_05_18_20_42_30 Train loss: 0.018519235774874687 at step: 671 lr 9e-05\n",
      "2024_05_18_20_42_41 Train loss: 0.15734197199344635 at step: 672 lr 9e-05\n",
      "2024_05_18_20_42_50 Train loss: 0.03513640537858009 at step: 673 lr 9e-05\n",
      "2024_05_18_20_42_58 Train loss: 0.13470140099525452 at step: 674 lr 9e-05\n",
      "2024_05_18_20_43_05 Train loss: 0.3068148195743561 at step: 675 lr 9e-05\n",
      "2024_05_18_20_43_17 Train loss: 0.05712195485830307 at step: 676 lr 9e-05\n",
      "2024_05_18_20_43_29 Train loss: 0.10328177362680435 at step: 677 lr 9e-05\n",
      "2024_05_18_20_43_42 Train loss: 0.273032546043396 at step: 678 lr 9e-05\n",
      "2024_05_18_20_43_53 Train loss: 0.05522463470697403 at step: 679 lr 9e-05\n",
      "2024_05_18_20_44_01 Train loss: 0.13599589467048645 at step: 680 lr 9e-05\n",
      "2024_05_18_20_44_16 Train loss: 0.036262042820453644 at step: 681 lr 9e-05\n",
      "2024_05_18_20_44_17 Train loss: 0.0024837420787662268 at step: 682 lr 9e-05\n",
      "(Val @ epoch 21) acc: 0.9652173913043478; ap: 0.9967845911949686\n",
      "2024_05_18_20_44_57 Train loss: 0.02592041715979576 at step: 683 lr 9e-05\n",
      "2024_05_18_20_45_06 Train loss: 0.05593511462211609 at step: 684 lr 9e-05\n",
      "2024_05_18_20_45_18 Train loss: 0.027128983289003372 at step: 685 lr 9e-05\n",
      "2024_05_18_20_45_26 Train loss: 0.08120299130678177 at step: 686 lr 9e-05\n",
      "2024_05_18_20_45_38 Train loss: 0.038291703909635544 at step: 687 lr 9e-05\n",
      "2024_05_18_20_45_44 Train loss: 0.05510168522596359 at step: 688 lr 9e-05\n",
      "2024_05_18_20_45_52 Train loss: 0.018498243764042854 at step: 689 lr 9e-05\n",
      "2024_05_18_20_46_02 Train loss: 0.016255401074886322 at step: 690 lr 9e-05\n",
      "2024_05_18_20_46_12 Train loss: 0.04619865119457245 at step: 691 lr 9e-05\n",
      "2024_05_18_20_46_20 Train loss: 0.08853653073310852 at step: 692 lr 9e-05\n",
      "2024_05_18_20_46_30 Train loss: 0.02616068720817566 at step: 693 lr 9e-05\n",
      "2024_05_18_20_46_38 Train loss: 0.06784437596797943 at step: 694 lr 9e-05\n",
      "2024_05_18_20_46_45 Train loss: 0.20802243053913116 at step: 695 lr 9e-05\n",
      "2024_05_18_20_46_56 Train loss: 0.03134799376130104 at step: 696 lr 9e-05\n",
      "2024_05_18_20_47_02 Train loss: 0.15894639492034912 at step: 697 lr 9e-05\n",
      "2024_05_18_20_47_14 Train loss: 0.03886816278100014 at step: 698 lr 9e-05\n",
      "2024_05_18_20_47_21 Train loss: 0.106241375207901 at step: 699 lr 9e-05\n",
      "2024_05_18_20_47_27 Train loss: 0.05334433913230896 at step: 700 lr 9e-05\n",
      "2024_05_18_20_47_35 Train loss: 0.09402256458997726 at step: 701 lr 9e-05\n",
      "2024_05_18_20_47_44 Train loss: 0.0678386315703392 at step: 702 lr 9e-05\n",
      "2024_05_18_20_47_53 Train loss: 0.05394834652543068 at step: 703 lr 9e-05\n",
      "2024_05_18_20_48_01 Train loss: 0.030486034229397774 at step: 704 lr 9e-05\n",
      "2024_05_18_20_48_08 Train loss: 0.08607902377843857 at step: 705 lr 9e-05\n",
      "2024_05_18_20_48_19 Train loss: 0.08964808285236359 at step: 706 lr 9e-05\n",
      "2024_05_18_20_48_26 Train loss: 0.019806664437055588 at step: 707 lr 9e-05\n",
      "2024_05_18_20_48_37 Train loss: 0.16584041714668274 at step: 708 lr 9e-05\n",
      "2024_05_18_20_48_44 Train loss: 0.09838403761386871 at step: 709 lr 9e-05\n",
      "2024_05_18_20_48_54 Train loss: 0.14250841736793518 at step: 710 lr 9e-05\n",
      "2024_05_18_20_49_03 Train loss: 0.07697919756174088 at step: 711 lr 9e-05\n",
      "2024_05_18_20_49_09 Train loss: 0.13890697062015533 at step: 712 lr 9e-05\n",
      "2024_05_18_20_49_10 Train loss: 0.059803739190101624 at step: 713 lr 9e-05\n",
      "(Val @ epoch 22) acc: 0.9739130434782609; ap: 0.9966885937782994\n",
      "2024_05_18_20_50_02 Train loss: 0.04629284888505936 at step: 714 lr 9e-05\n",
      "2024_05_18_20_50_20 Train loss: 0.21116343140602112 at step: 715 lr 9e-05\n",
      "2024_05_18_20_50_31 Train loss: 0.02442358434200287 at step: 716 lr 9e-05\n",
      "2024_05_18_20_50_41 Train loss: 0.07337597012519836 at step: 717 lr 9e-05\n",
      "2024_05_18_20_50_48 Train loss: 0.01899707317352295 at step: 718 lr 9e-05\n",
      "2024_05_18_20_50_55 Train loss: 0.12799538671970367 at step: 719 lr 9e-05\n",
      "2024_05_18_20_51_04 Train loss: 0.04703286290168762 at step: 720 lr 9e-05\n",
      "2024_05_18_20_51_14 Train loss: 0.02853981778025627 at step: 721 lr 9e-05\n",
      "2024_05_18_20_51_24 Train loss: 0.031401704996824265 at step: 722 lr 9e-05\n",
      "2024_05_18_20_51_32 Train loss: 0.08864450454711914 at step: 723 lr 9e-05\n",
      "2024_05_18_20_51_41 Train loss: 0.05044974014163017 at step: 724 lr 9e-05\n",
      "2024_05_18_20_51_49 Train loss: 0.021912693977355957 at step: 725 lr 9e-05\n",
      "2024_05_18_20_51_55 Train loss: 0.11894600838422775 at step: 726 lr 9e-05\n",
      "2024_05_18_20_52_01 Train loss: 0.02721484564244747 at step: 727 lr 9e-05\n",
      "2024_05_18_20_52_11 Train loss: 0.051457688212394714 at step: 728 lr 9e-05\n",
      "2024_05_18_20_52_23 Train loss: 0.09806171804666519 at step: 729 lr 9e-05\n",
      "2024_05_18_20_52_33 Train loss: 0.0398004874587059 at step: 730 lr 9e-05\n",
      "2024_05_18_20_52_42 Train loss: 0.0823444277048111 at step: 731 lr 9e-05\n",
      "2024_05_18_20_52_53 Train loss: 0.052162542939186096 at step: 732 lr 9e-05\n",
      "2024_05_18_20_53_04 Train loss: 0.4058343768119812 at step: 733 lr 9e-05\n",
      "2024_05_18_20_53_18 Train loss: 0.2379002869129181 at step: 734 lr 9e-05\n",
      "2024_05_18_20_53_30 Train loss: 0.05505548417568207 at step: 735 lr 9e-05\n",
      "2024_05_18_20_53_40 Train loss: 0.02423812448978424 at step: 736 lr 9e-05\n",
      "2024_05_18_20_53_49 Train loss: 0.036845192313194275 at step: 737 lr 9e-05\n",
      "2024_05_18_20_53_59 Train loss: 0.09351307153701782 at step: 738 lr 9e-05\n",
      "2024_05_18_20_54_09 Train loss: 0.10090848803520203 at step: 739 lr 9e-05\n",
      "2024_05_18_20_54_21 Train loss: 0.03227370232343674 at step: 740 lr 9e-05\n",
      "2024_05_18_20_54_33 Train loss: 0.029259422793984413 at step: 741 lr 9e-05\n",
      "2024_05_18_20_54_46 Train loss: 0.17794448137283325 at step: 742 lr 9e-05\n",
      "2024_05_18_20_54_56 Train loss: 0.05580846592783928 at step: 743 lr 9e-05\n",
      "2024_05_18_20_54_57 Train loss: 0.10698281228542328 at step: 744 lr 9e-05\n",
      "(Val @ epoch 23) acc: 0.9739130434782609; ap: 0.9966131907308378\n",
      "2024_05_18_20_55_27 Train loss: 0.13760076463222504 at step: 745 lr 9e-05\n",
      "2024_05_18_20_55_38 Train loss: 0.053692154586315155 at step: 746 lr 9e-05\n",
      "2024_05_18_20_55_48 Train loss: 0.03895733505487442 at step: 747 lr 9e-05\n",
      "2024_05_18_20_56_05 Train loss: 0.1744871884584427 at step: 748 lr 9e-05\n",
      "2024_05_18_20_56_18 Train loss: 0.027757981792092323 at step: 749 lr 9e-05\n",
      "2024_05_18_20_56_27 Train loss: 0.03968096524477005 at step: 750 lr 9e-05\n",
      "2024_05_18_20_56_38 Train loss: 0.013255773112177849 at step: 751 lr 9e-05\n",
      "2024_05_18_20_56_55 Train loss: 0.06497947126626968 at step: 752 lr 9e-05\n",
      "2024_05_18_20_57_04 Train loss: 0.04491673409938812 at step: 753 lr 9e-05\n",
      "2024_05_18_20_57_14 Train loss: 0.11223412305116653 at step: 754 lr 9e-05\n",
      "2024_05_18_20_57_24 Train loss: 0.03127361088991165 at step: 755 lr 9e-05\n",
      "2024_05_18_20_57_32 Train loss: 0.19819390773773193 at step: 756 lr 9e-05\n",
      "2024_05_18_20_57_41 Train loss: 0.0360461063683033 at step: 757 lr 9e-05\n",
      "2024_05_18_20_57_50 Train loss: 0.11919201910495758 at step: 758 lr 9e-05\n",
      "2024_05_18_20_58_03 Train loss: 0.24835684895515442 at step: 759 lr 9e-05\n",
      "2024_05_18_20_58_12 Train loss: 0.01562417484819889 at step: 760 lr 9e-05\n",
      "2024_05_18_20_58_23 Train loss: 0.011952303349971771 at step: 761 lr 9e-05\n",
      "2024_05_18_20_58_33 Train loss: 0.07042862474918365 at step: 762 lr 9e-05\n",
      "2024_05_18_20_58_38 Train loss: 0.44354695081710815 at step: 763 lr 9e-05\n",
      "2024_05_18_20_58_49 Train loss: 0.09025207161903381 at step: 764 lr 9e-05\n",
      "2024_05_18_20_59_01 Train loss: 0.014076280407607555 at step: 765 lr 9e-05\n",
      "2024_05_18_20_59_09 Train loss: 0.026270141825079918 at step: 766 lr 9e-05\n",
      "2024_05_18_20_59_16 Train loss: 0.07168594002723694 at step: 767 lr 9e-05\n",
      "2024_05_18_20_59_25 Train loss: 0.1787453293800354 at step: 768 lr 9e-05\n",
      "2024_05_18_20_59_32 Train loss: 0.024730462580919266 at step: 769 lr 9e-05\n",
      "2024_05_18_20_59_39 Train loss: 0.04138558357954025 at step: 770 lr 9e-05\n",
      "2024_05_18_20_59_49 Train loss: 0.028161969035863876 at step: 771 lr 9e-05\n",
      "2024_05_18_21_00_01 Train loss: 0.07560446858406067 at step: 772 lr 9e-05\n",
      "2024_05_18_21_00_12 Train loss: 0.16014805436134338 at step: 773 lr 9e-05\n",
      "2024_05_18_21_00_20 Train loss: 0.029461920261383057 at step: 774 lr 9e-05\n",
      "2024_05_18_21_00_21 Train loss: 0.015197386965155602 at step: 775 lr 9e-05\n",
      "(Val @ epoch 24) acc: 0.9826086956521739; ap: 1.0\n",
      "2024_05_18_21_00_55 Train loss: 0.12101415544748306 at step: 776 lr 9e-05\n",
      "2024_05_18_21_01_03 Train loss: 0.022872302681207657 at step: 777 lr 9e-05\n",
      "2024_05_18_21_01_13 Train loss: 0.07919449359178543 at step: 778 lr 9e-05\n",
      "2024_05_18_21_01_22 Train loss: 0.025208313018083572 at step: 779 lr 9e-05\n",
      "2024_05_18_21_01_31 Train loss: 0.081880584359169 at step: 780 lr 9e-05\n",
      "2024_05_18_21_01_40 Train loss: 0.06714547425508499 at step: 781 lr 9e-05\n",
      "2024_05_18_21_01_51 Train loss: 0.13613730669021606 at step: 782 lr 9e-05\n",
      "2024_05_18_21_02_02 Train loss: 0.07121460139751434 at step: 783 lr 9e-05\n",
      "2024_05_18_21_02_10 Train loss: 0.06932681798934937 at step: 784 lr 9e-05\n",
      "2024_05_18_21_02_19 Train loss: 0.013335286639630795 at step: 785 lr 9e-05\n",
      "2024_05_18_21_02_31 Train loss: 0.10993252694606781 at step: 786 lr 9e-05\n",
      "2024_05_18_21_02_40 Train loss: 0.12714138627052307 at step: 787 lr 9e-05\n",
      "2024_05_18_21_02_50 Train loss: 0.014719422906637192 at step: 788 lr 9e-05\n",
      "2024_05_18_21_03_00 Train loss: 0.11488820612430573 at step: 789 lr 9e-05\n",
      "2024_05_18_21_03_10 Train loss: 0.2193569540977478 at step: 790 lr 9e-05\n",
      "2024_05_18_21_03_20 Train loss: 0.03232148662209511 at step: 791 lr 9e-05\n",
      "2024_05_18_21_03_28 Train loss: 0.29780057072639465 at step: 792 lr 9e-05\n",
      "2024_05_18_21_03_34 Train loss: 0.04853207617998123 at step: 793 lr 9e-05\n",
      "2024_05_18_21_03_43 Train loss: 0.04233095049858093 at step: 794 lr 9e-05\n",
      "2024_05_18_21_03_53 Train loss: 0.08387751877307892 at step: 795 lr 9e-05\n",
      "2024_05_18_21_04_03 Train loss: 0.1064842939376831 at step: 796 lr 9e-05\n",
      "2024_05_18_21_04_12 Train loss: 0.021382052451372147 at step: 797 lr 9e-05\n",
      "2024_05_18_21_04_23 Train loss: 0.10713478922843933 at step: 798 lr 9e-05\n",
      "2024_05_18_21_04_35 Train loss: 0.12081652879714966 at step: 799 lr 9e-05\n",
      "2024_05_18_21_04_45 Train loss: 0.04671590030193329 at step: 800 lr 9e-05\n",
      "2024_05_18_21_04_55 Train loss: 0.097794309258461 at step: 801 lr 9e-05\n",
      "2024_05_18_21_05_04 Train loss: 0.023996390402317047 at step: 802 lr 9e-05\n",
      "2024_05_18_21_05_12 Train loss: 0.053444672375917435 at step: 803 lr 9e-05\n",
      "2024_05_18_21_05_28 Train loss: 0.05722168833017349 at step: 804 lr 9e-05\n",
      "2024_05_18_21_05_43 Train loss: 0.07588952034711838 at step: 805 lr 9e-05\n",
      "2024_05_18_21_05_46 Train loss: 0.36846381425857544 at step: 806 lr 9e-05\n",
      "(Val @ epoch 25) acc: 0.9652173913043478; ap: 0.9977771539651902\n",
      "2024_05_18_21_06_37 Train loss: 0.01034074928611517 at step: 807 lr 9e-05\n",
      "2024_05_18_21_06_45 Train loss: 0.14644573628902435 at step: 808 lr 9e-05\n",
      "2024_05_18_21_06_55 Train loss: 0.03159897029399872 at step: 809 lr 9e-05\n",
      "2024_05_18_21_07_05 Train loss: 0.026039771735668182 at step: 810 lr 9e-05\n",
      "2024_05_18_21_07_14 Train loss: 0.06234003230929375 at step: 811 lr 9e-05\n",
      "2024_05_18_21_07_24 Train loss: 0.12383034080266953 at step: 812 lr 9e-05\n",
      "2024_05_18_21_07_37 Train loss: 0.25833937525749207 at step: 813 lr 9e-05\n",
      "2024_05_18_21_07_44 Train loss: 0.07816191017627716 at step: 814 lr 9e-05\n",
      "2024_05_18_21_07_51 Train loss: 0.06178903207182884 at step: 815 lr 9e-05\n",
      "2024_05_18_21_08_02 Train loss: 0.10360794514417648 at step: 816 lr 9e-05\n",
      "2024_05_18_21_08_11 Train loss: 0.04105399176478386 at step: 817 lr 9e-05\n",
      "2024_05_18_21_08_18 Train loss: 0.17348447442054749 at step: 818 lr 9e-05\n",
      "2024_05_18_21_08_33 Train loss: 0.19174328446388245 at step: 819 lr 9e-05\n",
      "2024_05_18_21_08_44 Train loss: 0.09335833787918091 at step: 820 lr 9e-05\n",
      "2024_05_18_21_08_51 Train loss: 0.0980980396270752 at step: 821 lr 9e-05\n",
      "2024_05_18_21_09_04 Train loss: 0.1792643964290619 at step: 822 lr 9e-05\n",
      "2024_05_18_21_09_14 Train loss: 0.056447580456733704 at step: 823 lr 9e-05\n",
      "2024_05_18_21_09_21 Train loss: 0.02048901654779911 at step: 824 lr 9e-05\n",
      "2024_05_18_21_09_35 Train loss: 0.12337689101696014 at step: 825 lr 9e-05\n",
      "2024_05_18_21_09_49 Train loss: 0.08634163439273834 at step: 826 lr 9e-05\n",
      "2024_05_18_21_10_05 Train loss: 0.2143518477678299 at step: 827 lr 9e-05\n",
      "2024_05_18_21_10_24 Train loss: 0.06249389797449112 at step: 828 lr 9e-05\n",
      "2024_05_18_21_10_40 Train loss: 0.02440807968378067 at step: 829 lr 9e-05\n",
      "2024_05_18_21_10_51 Train loss: 0.08011012524366379 at step: 830 lr 9e-05\n",
      "2024_05_18_21_10_59 Train loss: 0.24748879671096802 at step: 831 lr 9e-05\n",
      "2024_05_18_21_11_09 Train loss: 0.09943094849586487 at step: 832 lr 9e-05\n",
      "2024_05_18_21_11_18 Train loss: 0.02927754819393158 at step: 833 lr 9e-05\n",
      "2024_05_18_21_11_32 Train loss: 0.2007824182510376 at step: 834 lr 9e-05\n",
      "2024_05_18_21_11_41 Train loss: 0.04347101226449013 at step: 835 lr 9e-05\n",
      "2024_05_18_21_11_51 Train loss: 0.020937195047736168 at step: 836 lr 9e-05\n",
      "2024_05_18_21_11_51 Train loss: 0.07178005576133728 at step: 837 lr 9e-05\n",
      "(Val @ epoch 26) acc: 0.9565217391304348; ap: 0.9974119562659269\n",
      "2024_05_18_21_12_22 Train loss: 0.1095782145857811 at step: 838 lr 9e-05\n",
      "2024_05_18_21_12_30 Train loss: 0.049504950642585754 at step: 839 lr 9e-05\n",
      "2024_05_18_21_12_38 Train loss: 0.15662051737308502 at step: 840 lr 9e-05\n",
      "2024_05_18_21_12_47 Train loss: 0.025105811655521393 at step: 841 lr 9e-05\n",
      "2024_05_18_21_12_57 Train loss: 0.10565537214279175 at step: 842 lr 9e-05\n",
      "2024_05_18_21_13_06 Train loss: 0.02508976310491562 at step: 843 lr 9e-05\n",
      "2024_05_18_21_13_17 Train loss: 0.2087797373533249 at step: 844 lr 9e-05\n",
      "2024_05_18_21_13_26 Train loss: 0.019983354955911636 at step: 845 lr 9e-05\n",
      "2024_05_18_21_13_35 Train loss: 0.24322298169136047 at step: 846 lr 9e-05\n",
      "2024_05_18_21_13_42 Train loss: 0.05403648689389229 at step: 847 lr 9e-05\n",
      "2024_05_18_21_13_53 Train loss: 0.0152231240645051 at step: 848 lr 9e-05\n",
      "2024_05_18_21_14_04 Train loss: 0.09988223761320114 at step: 849 lr 9e-05\n",
      "2024_05_18_21_14_13 Train loss: 0.281464546918869 at step: 850 lr 9e-05\n",
      "2024_05_18_21_14_23 Train loss: 0.11107632517814636 at step: 851 lr 9e-05\n",
      "2024_05_18_21_14_31 Train loss: 0.03565540909767151 at step: 852 lr 9e-05\n",
      "2024_05_18_21_14_42 Train loss: 0.050541918724775314 at step: 853 lr 9e-05\n",
      "2024_05_18_21_14_50 Train loss: 0.01901155151426792 at step: 854 lr 9e-05\n",
      "2024_05_18_21_14_58 Train loss: 0.11866109818220139 at step: 855 lr 9e-05\n",
      "2024_05_18_21_15_09 Train loss: 0.03774690628051758 at step: 856 lr 9e-05\n",
      "2024_05_18_21_15_15 Train loss: 0.07876451313495636 at step: 857 lr 9e-05\n",
      "2024_05_18_21_15_26 Train loss: 0.04898838698863983 at step: 858 lr 9e-05\n",
      "2024_05_18_21_15_36 Train loss: 0.04380812495946884 at step: 859 lr 9e-05\n",
      "2024_05_18_21_15_45 Train loss: 0.019576966762542725 at step: 860 lr 9e-05\n",
      "2024_05_18_21_15_54 Train loss: 0.17243345081806183 at step: 861 lr 9e-05\n",
      "2024_05_18_21_17_42 Train loss: 0.046495817601680756 at step: 870 lr 9e-05\n",
      "2024_05_18_21_17_52 Train loss: 0.014398341998457909 at step: 871 lr 9e-05\n",
      "2024_05_18_21_18_03 Train loss: 0.04399643838405609 at step: 872 lr 9e-05\n",
      "2024_05_18_21_18_15 Train loss: 0.1732710897922516 at step: 873 lr 9e-05\n",
      "2024_05_18_21_18_22 Train loss: 0.032489486038684845 at step: 874 lr 9e-05\n",
      "2024_05_18_21_18_30 Train loss: 0.019255265593528748 at step: 875 lr 9e-05\n",
      "2024_05_18_21_18_41 Train loss: 0.044071417301893234 at step: 876 lr 9e-05\n",
      "2024_05_18_21_18_50 Train loss: 0.031199507415294647 at step: 877 lr 9e-05\n",
      "2024_05_18_21_18_58 Train loss: 0.05278465524315834 at step: 878 lr 9e-05\n",
      "2024_05_18_21_19_05 Train loss: 0.0527334064245224 at step: 879 lr 9e-05\n",
      "2024_05_18_21_19_16 Train loss: 0.09001278132200241 at step: 880 lr 9e-05\n",
      "2024_05_18_21_19_23 "
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "opt = TrainOptions().parse()\n",
    "seed_torch(100)\n",
    "\n",
    "Logger(os.path.join(opt.checkpoints_dir, opt.name, 'log.log'))\n",
    "val_opt = get_val_opt()\n",
    "Testopt = TestOptions().parse(print_options=False)\n",
    "\n",
    "train_writer = SummaryWriter(os.path.join(opt.checkpoints_dir, opt.name, \"train\"))\n",
    "val_writer = SummaryWriter(os.path.join(opt.checkpoints_dir, opt.name, \"val\"))\n",
    "opt.earlystop_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f24d4101635363f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:24:13.013546Z",
     "start_time": "2024-05-16T00:24:12.990847Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Trainer(opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd198f74b2effc5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:24:13.016348Z",
     "start_time": "2024-05-16T00:24:13.014757Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=lambda d: tuple(d))\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=lambda d: tuple(d))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=lambda d: tuple(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f98e0f5e24e6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:28:15.618269Z",
     "start_time": "2024-05-16T00:24:13.018797Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/bmsn/lib/python3.10/site-packages/PIL/Image.py:3218: DecompressionBombWarning: Image size (119680000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "\n",
    "model.train()\n",
    "print(f'cwd: {os.getcwd()}')\n",
    "for epoch in range(opt.niter):\n",
    "    epoch_start_time = time.time()\n",
    "    iter_data_time = time.time()\n",
    "    epoch_iter = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        model.total_steps += 1\n",
    "        epoch_iter += opt.batch_size\n",
    "\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters()\n",
    "\n",
    "        print(time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime()), \"Train loss: {} at step: {} lr {}\".format(model.loss, model.total_steps, model.lr))\n",
    "    \n",
    "        if model.total_steps % opt.loss_freq == 0:\n",
    "            train_writer.add_scalar('loss', model.loss, model.total_steps)\n",
    "        \n",
    "    if epoch % opt.delr_freq == 0 and epoch != 0:\n",
    "        print(time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime()), 'changing lr at the end of epoch %d, iters %d' %\n",
    "              (epoch, model.total_steps))\n",
    "        model.adjust_learning_rate()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    acc, ap = validate(model.model, val_loader)[:2]\n",
    "    val_writer.add_scalar('accuracy', acc, model.total_steps)\n",
    "    val_writer.add_scalar('ap', ap, model.total_steps)\n",
    "    print(\"(Val @ epoch {}) acc: {}; ap: {}\".format(epoch, acc, ap))\n",
    "    if acc > best_val_acc:\n",
    "        model.save_networks('best')\n",
    "        best_val_acc = acc\n",
    "    #testmodel()\n",
    "    model.train()\n",
    "\n",
    "model.eval()\n",
    "acc, ap = validate(model.model, test_loader)[:2]\n",
    "print(\"(Test) acc: {}; ap: {}\".format(acc, ap))\n",
    "model.save_networks('last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59667ca6-3eb1-4d28-b72a-c473f3c1e4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmsn",
   "language": "python",
   "name": "bmsn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
